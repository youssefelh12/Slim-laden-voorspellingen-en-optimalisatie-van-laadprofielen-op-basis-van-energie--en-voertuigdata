{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.preprocessing\n",
    "from sklearn.metrics import r2_score\n",
    "from pandas import read_csv\n",
    "from datetime import datetime\n",
    "from keras.layers import Dense,Dropout,SimpleRNN,LSTM\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hour</th>\n",
       "      <th>Chargers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-28 12:00:00</td>\n",
       "      <td>2.4090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-09-28 13:00:00</td>\n",
       "      <td>6.0426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-09-28 14:00:00</td>\n",
       "      <td>6.0271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-09-28 15:00:00</td>\n",
       "      <td>6.0016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-09-28 16:00:00</td>\n",
       "      <td>7.1028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Hour  Chargers\n",
       "0  2022-09-28 12:00:00    2.4090\n",
       "1  2022-09-28 13:00:00    6.0426\n",
       "2  2022-09-28 14:00:00    6.0271\n",
       "3  2022-09-28 15:00:00    6.0016\n",
       "4  2022-09-28 16:00:00    7.1028"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../hourly_charging_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing rows:\n",
      "                     Chargers\n",
      "2022-10-05 13:00:00       NaN\n",
      "2022-10-05 14:00:00       NaN\n",
      "2022-10-05 15:00:00       NaN\n",
      "2022-10-05 16:00:00       NaN\n",
      "2022-10-05 17:00:00       NaN\n",
      "...                       ...\n",
      "2023-09-06 14:00:00       NaN\n",
      "2023-09-06 15:00:00       NaN\n",
      "2023-09-06 16:00:00       NaN\n",
      "2024-03-31 02:00:00       NaN\n",
      "2024-05-22 17:00:00       NaN\n",
      "\n",
      "[1042 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12976\\725497301.py:8: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  all_hours = pd.date_range(start=df.index.min(), end=df.index.max(), freq='H')\n"
     ]
    }
   ],
   "source": [
    "# 1. Convert Hour to DateTime type\n",
    "df['Hour'] = pd.to_datetime(df['Hour'])\n",
    "\n",
    "# 2. Set Hour as the DataFrame index\n",
    "df = df.set_index('Hour')\n",
    "\n",
    "# 3. Reindex to every hour in the range from the min to max timestamps\n",
    "all_hours = pd.date_range(start=df.index.min(), end=df.index.max(), freq='H')\n",
    "df_reindexed = df.reindex(all_hours)\n",
    "\n",
    "# 4. Identify which rows are missing\n",
    "missing_rows = df_reindexed[df_reindexed['Chargers'].isnull()]\n",
    "print(\"Missing rows:\")\n",
    "print(missing_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chargers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hour</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-09-28 12:00:00</th>\n",
       "      <td>2.4090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-28 13:00:00</th>\n",
       "      <td>6.0426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-28 14:00:00</th>\n",
       "      <td>6.0271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-28 15:00:00</th>\n",
       "      <td>6.0016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-28 16:00:00</th>\n",
       "      <td>7.1028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-19 08:00:00</th>\n",
       "      <td>45.9840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-19 09:00:00</th>\n",
       "      <td>106.6935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-19 10:00:00</th>\n",
       "      <td>101.6063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-19 11:00:00</th>\n",
       "      <td>84.9264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-19 12:00:00</th>\n",
       "      <td>51.2582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19959 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Chargers\n",
       "Hour                         \n",
       "2022-09-28 12:00:00    2.4090\n",
       "2022-09-28 13:00:00    6.0426\n",
       "2022-09-28 14:00:00    6.0271\n",
       "2022-09-28 15:00:00    6.0016\n",
       "2022-09-28 16:00:00    7.1028\n",
       "...                       ...\n",
       "2025-02-19 08:00:00   45.9840\n",
       "2025-02-19 09:00:00  106.6935\n",
       "2025-02-19 10:00:00  101.6063\n",
       "2025-02-19 11:00:00   84.9264\n",
       "2025-02-19 12:00:00   51.2582\n",
       "\n",
       "[19959 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a string key for month-day-hour, e.g. '09-06-03'\n",
    "df_reindexed['month_day_hour'] = df_reindexed.index.strftime('%m-%d-%H')\n",
    "\n",
    "# Compute the mean for each specific month/day/hour across all years\n",
    "mdh_mean = df_reindexed.groupby('month_day_hour')['Chargers'].transform('mean')\n",
    "\n",
    "# Fill missing values with that mean\n",
    "df['Chargers'] = df_reindexed['Chargers'].fillna(mdh_mean)\n",
    "\n",
    "# Clean up the extra grouping column\n",
    "df_reindexed.drop(columns=['month_day_hour'], inplace=True)\n",
    "\n",
    "df.isna().sum()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "df_filtered = df.iloc[10000:20000]\n",
    "\n",
    "results = seasonal_decompose(df_filtered['Chargers'])\n",
    "results.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chargers    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dropping rows with missing df\n",
    "df = df.dropna()\n",
    "#checking missing data\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved → ..\\models\\chargers\\charger_scaler.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\enviroments\\Stage_project\\.venv\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "62/62 - 29s - 468ms/step - loss: 0.0152 - val_loss: 0.0472\n",
      "Epoch 2/50\n",
      "62/62 - 26s - 414ms/step - loss: 0.0105 - val_loss: 0.0356\n",
      "Epoch 3/50\n",
      "62/62 - 30s - 484ms/step - loss: 0.0093 - val_loss: 0.0288\n",
      "Epoch 4/50\n",
      "62/62 - 29s - 472ms/step - loss: 0.0081 - val_loss: 0.0255\n",
      "Epoch 5/50\n",
      "62/62 - 29s - 470ms/step - loss: 0.0071 - val_loss: 0.0261\n",
      "Epoch 6/50\n",
      "62/62 - 30s - 490ms/step - loss: 0.0067 - val_loss: 0.0237\n",
      "Epoch 7/50\n",
      "62/62 - 30s - 477ms/step - loss: 0.0063 - val_loss: 0.0248\n",
      "Epoch 8/50\n",
      "62/62 - 33s - 529ms/step - loss: 0.0058 - val_loss: 0.0258\n",
      "Epoch 9/50\n",
      "62/62 - 30s - 491ms/step - loss: 0.0055 - val_loss: 0.0282\n",
      "Epoch 10/50\n",
      "62/62 - 31s - 502ms/step - loss: 0.0051 - val_loss: 0.0268\n",
      "Epoch 11/50\n",
      "62/62 - 30s - 487ms/step - loss: 0.0048 - val_loss: 0.0262\n",
      "Epoch 12/50\n",
      "62/62 - 31s - 492ms/step - loss: 0.0045 - val_loss: 0.0233\n",
      "Epoch 13/50\n",
      "62/62 - 31s - 499ms/step - loss: 0.0042 - val_loss: 0.0281\n",
      "Epoch 14/50\n",
      "62/62 - 30s - 489ms/step - loss: 0.0041 - val_loss: 0.0240\n",
      "Epoch 15/50\n",
      "62/62 - 30s - 484ms/step - loss: 0.0039 - val_loss: 0.0253\n",
      "Epoch 16/50\n",
      "62/62 - 31s - 495ms/step - loss: 0.0039 - val_loss: 0.0255\n",
      "Epoch 17/50\n",
      "62/62 - 31s - 500ms/step - loss: 0.0038 - val_loss: 0.0273\n",
      "Epoch 18/50\n",
      "62/62 - 31s - 496ms/step - loss: 0.0037 - val_loss: 0.0261\n",
      "Epoch 19/50\n",
      "62/62 - 30s - 488ms/step - loss: 0.0037 - val_loss: 0.0259\n",
      "Epoch 20/50\n",
      "62/62 - 30s - 492ms/step - loss: 0.0036 - val_loss: 0.0273\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 73ms/step\n",
      "TEST 24-step →  RMSE=0.176  MAE=0.089  R²=0.613\n",
      "Model saved → ..\\models\\chargers\\lstm_chargers.keras\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Multivariate, 24-step LSTM – trains with a 168-hour look-back,\n",
    "saves both the network and the fitted MinMaxScaler.\n",
    "\"\"\"\n",
    "\n",
    "import pathlib, math, holidays, joblib    # ← NEW: joblib\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ───────────────────────── CONFIG ──────────────────────────\n",
    "TARGET_COL   = \"Chargers\"\n",
    "LOOK_BACK    = 72          # 7 days window (168 × 1 h)\n",
    "N_FORECAST   = 24             # predict 24 h ahead\n",
    "EPOCHS       = 50\n",
    "BATCH_SIZE   = 256\n",
    "PATIENCE     = 8\n",
    "MODEL_PATH   = pathlib.Path(\"../models/chargers/lstm_chargers.keras\")\n",
    "SCALER_PATH  = pathlib.Path(\"../models/chargers/charger_scaler.joblib\")  # ← NEW\n",
    "START_DATE   = \"2022-09-11\"\n",
    "END_DATE     = \"2025-02-19\"\n",
    "\n",
    "# ───────────────────────── DATA LOAD ───────────────────────\n",
    "df = df.loc[START_DATE:END_DATE].copy()      # replace with your loader\n",
    "\n",
    "# ─────────────────────── FEATURE ENGINEERING ───────────────\n",
    "be_holidays = set(holidays.country_holidays(\n",
    "                  \"BE\", years=[2022, 2023, 2024, 2025]).keys())\n",
    "\n",
    "def add_terugkomdag_feature(df):\n",
    "    # List of 'terugkomdagen' dates\n",
    "    terugkomdagen = [\n",
    "        datetime(2023, 9, 13), datetime(2023, 10, 26), datetime(2023, 11, 14), datetime(2023, 12, 20),\n",
    "        datetime(2024, 1, 12), datetime(2024, 2, 7), datetime(2024, 3, 14), datetime(2024, 4, 16),\n",
    "        datetime(2024, 5, 13), datetime(2024, 6, 7), datetime(2024, 3, 16), datetime(2024, 10, 22),\n",
    "        datetime(2024, 11, 28), datetime(2024, 12, 18), datetime(2025, 1, 10), datetime(2025, 2, 13),\n",
    "        datetime(2025, 3, 18), datetime(2025, 4, 22), datetime(2025, 5, 12), datetime(2025, 6, 6)\n",
    "    ]\n",
    "    df['is_terugkomdag'] = df.index.to_series().dt.date.isin([d.date() for d in terugkomdagen]).astype(int)\n",
    "\n",
    "    return df\n",
    "def add_cumulative_ev_phev_feature(df):\n",
    "    from datetime import datetime\n",
    "\n",
    "    # List of (date, cumulative_count) from your analysis\n",
    "    cumulative_data = {\n",
    "        datetime(2024, 6, 20): 35,\n",
    "        datetime(2024, 6, 25): 36,\n",
    "        datetime(2024, 9, 5): 38,\n",
    "        datetime(2024, 9, 12): 41,\n",
    "        datetime(2024, 9, 27): 42,\n",
    "        datetime(2024, 10, 15): 43,\n",
    "        datetime(2024, 10, 29): 45,\n",
    "        datetime(2024, 11, 5): 46,\n",
    "        datetime(2024, 11, 26): 47,\n",
    "        datetime(2025, 1, 9): 48,\n",
    "        datetime(2025, 1, 23): 49,\n",
    "        datetime(2025, 1, 28): 50,\n",
    "        datetime(2025, 2, 4): 51,\n",
    "    }\n",
    "\n",
    "    # Turn it into a Series and reindex to all dates in your dataset\n",
    "    ev_series = pd.Series(cumulative_data)\n",
    "    ev_series = ev_series.reindex(df.index.union(ev_series.index)).sort_index().ffill().fillna(0)\n",
    "\n",
    "    # Add it to your DataFrame\n",
    "    df[\"cumulative_ev_phev_count\"] = ev_series.reindex(df.index).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "add_terugkomdag_feature(df)\n",
    "add_cumulative_ev_phev_feature(df)\n",
    "\n",
    "df[\"hour\"]           = df.index.hour\n",
    "df[\"day_of_week\"]    = df.index.dayofweek\n",
    "df[\"month\"]          = df.index.month\n",
    "df[\"is_weekend\"]     = (df[\"day_of_week\"] >= 5).astype(int)\n",
    "df[\"is_festive\"]     = df.index.to_series().apply(\n",
    "                         lambda d: int(d.date() in be_holidays))\n",
    "df[\"working_hour\"]   = df[\"hour\"].between(8, 18).astype(int)\n",
    "df[\"is_summer\"]      = df[\"month\"].isin([6, 7, 8]).astype(int)\n",
    "df[\"is_winter\"]      = df[\"month\"].isin([12, 1, 2]).astype(int)\n",
    "df[\"is_morning_peak\"] = df[\"hour\"].between(7, 9).astype(int)\n",
    "df[\"is_evening_peak\"] = df[\"hour\"].between(17, 20).astype(int)\n",
    "df[\"hour_sin\"]       = np.sin(2*np.pi*df[\"hour\"]/24)\n",
    "df[\"hour_cos\"]       = np.cos(2*np.pi*df[\"hour\"]/24)\n",
    "df[\"dow_sin\"]        = np.sin(2*np.pi*df[\"day_of_week\"]/7)\n",
    "df[\"dow_cos\"]        = np.cos(2*np.pi*df[\"day_of_week\"]/7)\n",
    "\n",
    "FEATURE_COLS = [c for c in df.columns if c != TARGET_COL]\n",
    "\n",
    "# ─────────────── 1️⃣  CHRONOLOGICAL SPLIT  ────────────────\n",
    "train_size = int(len(df)*0.8)\n",
    "val_size   = int(len(df)*0.1)\n",
    "\n",
    "df_train = df.iloc[:train_size]\n",
    "df_val   = df.iloc[train_size:train_size+val_size]\n",
    "df_test  = df.iloc[train_size+val_size:]\n",
    "\n",
    "# ─────────────── 2️⃣  FIT & SAVE SCALER  ──────────────────\n",
    "scaler = MinMaxScaler()                                     # DOCS :contentReference[oaicite:2]{index=2}\n",
    "scaler.fit(df_train[[TARGET_COL] + FEATURE_COLS])           # fit **train only** :contentReference[oaicite:3]{index=3}\n",
    "joblib.dump(scaler, SCALER_PATH)                            # persist scaler :contentReference[oaicite:4]{index=4}\n",
    "print(\"Scaler saved →\", SCALER_PATH)\n",
    "\n",
    "def scale(frame):        # helper to apply the saved scaler\n",
    "    cols = [TARGET_COL] + FEATURE_COLS\n",
    "    return pd.DataFrame(scaler.transform(frame[cols]), columns=cols,\n",
    "                        index=frame.index)\n",
    "\n",
    "df_train_s, df_val_s, df_test_s = map(scale, (df_train, df_val, df_test))\n",
    "\n",
    "# ─────────────── 3️⃣  BUILD INPUT / LABEL WINDOWS ─────────\n",
    "def make_xy(frame, look_back, horizon):\n",
    "    data = frame[[TARGET_COL]+FEATURE_COLS].values\n",
    "    X, y = [], []\n",
    "    for i in range(look_back, len(data)-horizon+1):\n",
    "        X.append(data[i-look_back:i])\n",
    "        y.append(data[i:i+horizon, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = make_xy(df_train_s, LOOK_BACK, N_FORECAST)\n",
    "X_val,   y_val   = make_xy(df_val_s,   LOOK_BACK, N_FORECAST)\n",
    "X_test,  y_test  = make_xy(df_test_s,  LOOK_BACK, N_FORECAST)\n",
    "\n",
    "# ─────────────── 4️⃣  DEFINE & TRAIN MODEL ────────────────\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(LOOK_BACK, X_train.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64),\n",
    "    Dropout(0.2),\n",
    "    Dense(N_FORECAST)\n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "cb = EarlyStopping(patience=PATIENCE, restore_best_weights=True)  # :contentReference[oaicite:5]{index=5}\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "          validation_data=(X_val, y_val), callbacks=[cb], verbose=2)\n",
    "\n",
    "# ─────────────── 5️⃣  EVALUATE  ───────────────────────────\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "rmse = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae  = mean_absolute_error(y_test, y_pred)\n",
    "r2   = r2_score(y_test, y_pred)\n",
    "print(f\"TEST 24-step →  RMSE={rmse:.3f}  MAE={mae:.3f}  R²={r2:.3f}\")\n",
    "\n",
    "# ─────────────── 6️⃣  SAVE MODEL (.keras format) ──────────\n",
    "MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "model.save(MODEL_PATH)                                       # Keras v3 format :contentReference[oaicite:6]{index=6}\n",
    "print(\"Model saved →\", MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved → ..\\models\\chargers\\charger_scaler_nofeat.joblib\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\enviroments\\Stage_project\\.venv\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 - 23s - 373ms/step - loss: 0.0169 - val_loss: 0.0602\n",
      "Epoch 2/50\n",
      "62/62 - 23s - 372ms/step - loss: 0.0129 - val_loss: 0.0494\n",
      "Epoch 3/50\n",
      "62/62 - 27s - 441ms/step - loss: 0.0116 - val_loss: 0.0464\n",
      "Epoch 4/50\n",
      "62/62 - 26s - 425ms/step - loss: 0.0111 - val_loss: 0.0440\n",
      "Epoch 5/50\n",
      "62/62 - 27s - 430ms/step - loss: 0.0105 - val_loss: 0.0415\n",
      "Epoch 6/50\n",
      "62/62 - 26s - 426ms/step - loss: 0.0100 - val_loss: 0.0422\n",
      "Epoch 7/50\n",
      "62/62 - 28s - 450ms/step - loss: 0.0095 - val_loss: 0.0425\n",
      "Epoch 8/50\n",
      "62/62 - 26s - 426ms/step - loss: 0.0091 - val_loss: 0.0420\n",
      "Epoch 9/50\n",
      "62/62 - 27s - 438ms/step - loss: 0.0086 - val_loss: 0.0413\n",
      "Epoch 10/50\n",
      "62/62 - 26s - 419ms/step - loss: 0.0083 - val_loss: 0.0420\n",
      "Epoch 11/50\n",
      "62/62 - 29s - 462ms/step - loss: 0.0081 - val_loss: 0.0410\n",
      "Epoch 12/50\n",
      "62/62 - 30s - 478ms/step - loss: 0.0078 - val_loss: 0.0399\n",
      "Epoch 13/50\n",
      "62/62 - 29s - 474ms/step - loss: 0.0076 - val_loss: 0.0376\n",
      "Epoch 14/50\n",
      "62/62 - 30s - 482ms/step - loss: 0.0075 - val_loss: 0.0388\n",
      "Epoch 15/50\n",
      "62/62 - 29s - 474ms/step - loss: 0.0073 - val_loss: 0.0390\n",
      "Epoch 16/50\n",
      "62/62 - 29s - 474ms/step - loss: 0.0071 - val_loss: 0.0366\n",
      "Epoch 17/50\n",
      "62/62 - 30s - 489ms/step - loss: 0.0071 - val_loss: 0.0365\n",
      "Epoch 18/50\n",
      "62/62 - 32s - 523ms/step - loss: 0.0069 - val_loss: 0.0386\n",
      "Epoch 19/50\n",
      "62/62 - 31s - 498ms/step - loss: 0.0069 - val_loss: 0.0361\n",
      "Epoch 20/50\n",
      "62/62 - 30s - 478ms/step - loss: 0.0069 - val_loss: 0.0371\n",
      "Epoch 21/50\n",
      "62/62 - 29s - 471ms/step - loss: 0.0067 - val_loss: 0.0366\n",
      "Epoch 22/50\n",
      "62/62 - 38s - 618ms/step - loss: 0.0066 - val_loss: 0.0364\n",
      "Epoch 23/50\n",
      "62/62 - 33s - 537ms/step - loss: 0.0065 - val_loss: 0.0362\n",
      "Epoch 24/50\n",
      "62/62 - 46s - 734ms/step - loss: 0.0066 - val_loss: 0.0375\n",
      "Epoch 25/50\n",
      "62/62 - 47s - 761ms/step - loss: 0.0063 - val_loss: 0.0372\n",
      "Epoch 26/50\n",
      "62/62 - 45s - 722ms/step - loss: 0.0063 - val_loss: 0.0364\n",
      "Epoch 27/50\n",
      "62/62 - 29s - 472ms/step - loss: 0.0062 - val_loss: 0.0382\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 101ms/step\n",
      "TEST 24-step →  RMSE=0.217  MAE=0.114  R²=0.414\n",
      "Model saved → ..\\models\\chargers\\lstm_chargers_nofeat.keras\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ───────────────────────── CONFIG ──────────────────────────\n",
    "TARGET_COL   = \"Chargers\"\n",
    "LOOK_BACK    = 72      # 72 hours of history\n",
    "N_FORECAST   = 24      # predict 24 hours ahead\n",
    "EPOCHS       = 50\n",
    "BATCH_SIZE   = 256\n",
    "PATIENCE     = 8\n",
    "MODEL_PATH   = pathlib.Path(\"../models/chargers/lstm_chargers_nofeat.keras\")\n",
    "SCALER_PATH  = pathlib.Path(\"../models/chargers/charger_scaler_nofeat.joblib\")\n",
    "START_DATE   = \"2022-09-11\"\n",
    "END_DATE     = \"2025-02-19\"\n",
    "\n",
    "# ───────────────────────── DATA LOAD ───────────────────────\n",
    "# (Replace this with your actual data-loading code)\n",
    "df = df.loc[START_DATE:END_DATE].copy()\n",
    "\n",
    "# ────────────────────────── PREPARATION ────────────────────\n",
    "FEATURE_COLS = []  # no extra features\n",
    "\n",
    "# ─────────────── 1️⃣  CHRONOLOGICAL SPLIT  ────────────────\n",
    "train_size = int(len(df) * 0.8)\n",
    "val_size   = int(len(df) * 0.1)\n",
    "\n",
    "df_train = df.iloc[:train_size]\n",
    "df_val   = df.iloc[train_size:train_size + val_size]\n",
    "df_test  = df.iloc[train_size + val_size:]\n",
    "\n",
    "# ─────────────── 2️⃣  FIT & SAVE SCALER  ──────────────────\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df_train[[TARGET_COL]])          # fit on training target only\n",
    "joblib.dump(scaler, SCALER_PATH)\n",
    "print(\"Scaler saved →\", SCALER_PATH)\n",
    "\n",
    "def scale(frame):\n",
    "    cols = [TARGET_COL]\n",
    "    scaled = scaler.transform(frame[cols])\n",
    "    return pd.DataFrame(scaled, columns=cols, index=frame.index)\n",
    "\n",
    "df_train_s = scale(df_train)\n",
    "df_val_s   = scale(df_val)\n",
    "df_test_s  = scale(df_test)\n",
    "\n",
    "# ─────────────── 3️⃣  BUILD INPUT / LABEL WINDOWS ─────────\n",
    "def make_xy(frame, look_back, horizon):\n",
    "    data = frame[[TARGET_COL]].values\n",
    "    X, y = [], []\n",
    "    for i in range(look_back, len(data) - horizon + 1):\n",
    "        X.append(data[i - look_back : i])\n",
    "        y.append(data[i : i + horizon, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = make_xy(df_train_s, LOOK_BACK, N_FORECAST)\n",
    "X_val,   y_val   = make_xy(df_val_s,   LOOK_BACK, N_FORECAST)\n",
    "X_test,  y_test  = make_xy(df_test_s,  LOOK_BACK, N_FORECAST)\n",
    "\n",
    "# ─────────────── 4️⃣  DEFINE & TRAIN MODEL ────────────────\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(LOOK_BACK, 1)),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64),\n",
    "    Dropout(0.2),\n",
    "    Dense(N_FORECAST)\n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "early_stop = EarlyStopping(patience=PATIENCE, restore_best_weights=True)\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stop],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# ─────────────── 5️⃣  EVALUATE  ───────────────────────────\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "rmse = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae  = mean_absolute_error(y_test, y_pred)\n",
    "r2   = r2_score(y_test, y_pred)\n",
    "print(f\"TEST 24-step →  RMSE={rmse:.3f}  MAE={mae:.3f}  R²={r2:.3f}\")\n",
    "\n",
    "# ─────────────── 6️⃣  SAVE MODEL (.keras format) ──────────\n",
    "MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "model.save(MODEL_PATH)\n",
    "print(\"Model saved →\", MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# TensorFlow imports (or swap in PyTorch, etc.)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, Dropout, Dense\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Specify the column you want to forecast\n",
    "# -------------------------------------------------------\n",
    "target_col = \"Chargers\"\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1. Filter data to desired date range\n",
    "# -------------------------------------------------------\n",
    "start_date = \"2022-09-11\"\n",
    "end_date   = \"2025-02-19\"\n",
    "df = df.loc[start_date:end_date]\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2. Visualize data before normalization (optional)\n",
    "# -------------------------------------------------------\n",
    "plt.figure(figsize=(16,4))\n",
    "df[target_col].plot(legend=True)\n",
    "plt.title(f'Hourly {target_col} data - BEFORE NORMALIZATION')\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3. Normalize data\n",
    "# -------------------------------------------------------\n",
    "def normalize_data(df, col):\n",
    "    scaler = MinMaxScaler()\n",
    "    df[col] = scaler.fit_transform(df[col].values.reshape(-1,1))\n",
    "    return df, scaler\n",
    "\n",
    "# Apply normalization\n",
    "df_norm, scaler = normalize_data(df.copy(), target_col)\n",
    "\n",
    "# Visualize after normalization (optional)\n",
    "plt.figure(figsize=(16,4))\n",
    "df_norm[target_col].plot(legend=True)\n",
    "plt.title(f'Hourly {target_col} data - AFTER NORMALIZATION')\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4. Create sequences from your normalized column\n",
    "# -------------------------------------------------------\n",
    "def create_sequences(data_arr, seq_len):\n",
    "    X, y = [], []\n",
    "    for i in range(seq_len, len(data_arr)):\n",
    "        X.append(data_arr[i - seq_len : i])\n",
    "        y.append(data_arr[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "seq_len = 20\n",
    "data_arr = df_norm[target_col].values\n",
    "X, y = create_sequences(data_arr, seq_len)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 5. Split into train, validation, and test sets\n",
    "# -------------------------------------------------------\n",
    "train_size = int(len(X) * 0.80)\n",
    "val_size   = int(len(X) * 0.10)\n",
    "\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val,   y_val   = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test,  y_test  = X[train_size+val_size:], y[train_size+val_size:]\n",
    "\n",
    "# Reshape data for RNN/LSTM: [samples, timesteps, features]\n",
    "X_train = X_train.reshape(-1, seq_len, 1)\n",
    "X_val   = X_val.reshape(-1, seq_len, 1)\n",
    "X_test  = X_test.reshape(-1, seq_len, 1)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 6. Build and train the RNN model\n",
    "# -------------------------------------------------------\n",
    "rnn_model = Sequential([\n",
    "    SimpleRNN(40, activation='tanh', return_sequences=True, input_shape=(seq_len,1)),\n",
    "    Dropout(0.15),\n",
    "    SimpleRNN(40, activation='tanh', return_sequences=True),\n",
    "    Dropout(0.15),\n",
    "    SimpleRNN(40, activation='tanh'),\n",
    "    Dropout(0.15),\n",
    "    Dense(1)\n",
    "])\n",
    "rnn_model.compile(optimizer='adam', loss='mse')\n",
    "rnn_model.summary()\n",
    "\n",
    "history_rnn = rnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=256,\n",
    "    validation_data=(X_val, y_val)\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 7. Evaluate RNN on test set\n",
    "# -------------------------------------------------------\n",
    "rnn_pred = rnn_model.predict(X_test)\n",
    "rnn_r2   = r2_score(y_test, rnn_pred)\n",
    "rnn_mae  = mean_absolute_error(y_test, rnn_pred)\n",
    "rnn_mse  = mean_squared_error(y_test, rnn_pred)\n",
    "rnn_rmse = np.sqrt(rnn_mse)\n",
    "\n",
    "print(f'RNN R2 score: {rnn_r2:.4f}')\n",
    "print(f'RNN MAE: {rnn_mae:.4f}')\n",
    "print(f'RNN MSE: {rnn_mse:.4f}')\n",
    "print(f'RNN RMSE: {rnn_rmse:.4f}')\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 8. Build and train the LSTM model\n",
    "# -------------------------------------------------------\n",
    "lstm_model = Sequential([\n",
    "    LSTM(40, activation='tanh', return_sequences=True, input_shape=(seq_len,1)),\n",
    "    Dropout(0.15),\n",
    "    LSTM(40, activation='tanh', return_sequences=True),\n",
    "    Dropout(0.15),\n",
    "    LSTM(40, activation='tanh'),\n",
    "    Dropout(0.15),\n",
    "    Dense(1)\n",
    "])\n",
    "lstm_model.compile(optimizer='adam', loss='mse')\n",
    "lstm_model.summary()\n",
    "\n",
    "history_lstm = lstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=256,\n",
    "    validation_data=(X_val, y_val)\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 9. Evaluate LSTM on test set\n",
    "# -------------------------------------------------------\n",
    "lstm_pred = lstm_model.predict(X_test)\n",
    "lstm_r2   = r2_score(y_test, lstm_pred)\n",
    "lstm_mae  = mean_absolute_error(y_test, lstm_pred)\n",
    "lstm_mse  = mean_squared_error(y_test, lstm_pred)\n",
    "lstm_rmse = np.sqrt(lstm_mse)\n",
    "\n",
    "print(f'LSTM R2 score: {lstm_r2:.4f}')\n",
    "print(f'LSTM MAE: {lstm_mae:.4f}')\n",
    "print(f'LSTM MSE: {lstm_mse:.4f}')\n",
    "print(f'LSTM RMSE: {lstm_rmse:.4f}')\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 10. Plot predictions vs actuals for both models\n",
    "# -------------------------------------------------------\n",
    "def plot_preds(actual, preds, labels, title):\n",
    "    plt.figure(figsize=(16,4))\n",
    "    plt.plot(actual, label='Actual', linewidth=2)\n",
    "    for pred, lbl in zip(preds, labels):\n",
    "        plt.plot(pred, alpha=0.7, label=lbl)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time steps')\n",
    "    plt.ylabel('Normalized')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_preds(\n",
    "    y_test,\n",
    "    preds=[rnn_pred, lstm_pred],\n",
    "    labels=['RNN Prediction', 'LSTM Prediction'],\n",
    "    title=f'{target_col} Predictions vs Actual'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# TensorFlow imports (or swap in PyTorch, etc.)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, Dropout, Dense\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Specify the column you want to forecast\n",
    "# -------------------------------------------------------\n",
    "target_col = \"Chargers\"\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1. Filter data to desired date range\n",
    "# -------------------------------------------------------\n",
    "start_date = \"2022-09-11\"\n",
    "end_date   = \"2025-02-19\"\n",
    "df = df.loc[start_date:end_date]\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2. Create temporal and categorical features\n",
    "# -------------------------------------------------------\n",
    "# Ensure you have a list or set of Belgian holidays as dates\n",
    "# Example: be_holidays = set(holidays.BE(years=[2022,2023,2024,2025]).keys())\n",
    "# -------------------------------------------------------\n",
    "df['hour'] = df.index.hour\n",
    "df['day_of_week'] = df.index.dayofweek\n",
    "df['month'] = df.index.month\n",
    "\n",
    "# Categorical features\n",
    "df['is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "#df['is_festive'] = df.index.to_series().apply(lambda x: 1 if x.date() in be_holidays else 0)\n",
    "df['working_hour'] = df['hour'].apply(lambda x: 1 if 8 <= x <= 18 else 0)\n",
    "\n",
    "# Seasonal features\n",
    "df['is_summer'] = df['month'].isin([6, 7, 8]).astype(int)\n",
    "df['is_winter'] = df['month'].isin([12, 1, 2]).astype(int)\n",
    "\n",
    "# Peak hour features\n",
    "df['is_morning_peak'] = ((df['hour'] >= 7) & (df['hour'] <= 9)).astype(int)\n",
    "df['is_evening_peak'] = ((df['hour'] >= 17) & (df['hour'] <= 20)).astype(int)\n",
    "\n",
    "# Cyclical features for hour and day of week\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3. Visualize data before normalization (optional)\n",
    "# -------------------------------------------------------\n",
    "plt.figure(figsize=(16,4))\n",
    "df[target_col].plot(legend=True)\n",
    "plt.title(f'Hourly {target_col} data - BEFORE NORMALIZATION')\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4. Normalize data\n",
    "# -------------------------------------------------------\n",
    "def normalize_data(df, col, feature_cols=None):\n",
    "    scaler = MinMaxScaler()\n",
    "    cols_to_scale = [col] + (feature_cols or [])\n",
    "    df[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])\n",
    "    return df, scaler\n",
    "\n",
    "# List of additional features to include if using multivariate input\n",
    "feature_cols = [\n",
    "    'hour', 'day_of_week', 'month', 'is_weekend', 'working_hour',\n",
    "    'is_summer', 'is_winter', 'is_morning_peak', 'is_evening_peak',\n",
    "    'hour_sin', 'hour_cos', 'day_of_week_sin', 'day_of_week_cos'\n",
    "]\n",
    "\n",
    "# Apply normalization\n",
    "df_norm, scaler = normalize_data(df.copy(), target_col, feature_cols)\n",
    "\n",
    "# Visualize after normalization (optional)\n",
    "plt.figure(figsize=(16,4))\n",
    "df_norm[target_col].plot(legend=True)\n",
    "plt.title(f'Hourly {target_col} data - AFTER NORMALIZATION')\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 5. Create sequences from your normalized data\n",
    "# -------------------------------------------------------\n",
    "def create_sequences(data_df, seq_len, target_col, feature_cols=None):\n",
    "    X, y = [], []\n",
    "    arr = data_df[[target_col] + (feature_cols or [])].values\n",
    "    for i in range(seq_len, len(arr)):\n",
    "        X.append(arr[i - seq_len:i])\n",
    "        y.append(arr[i, 0])  # first column is target\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "seq_len = 20\n",
    "X, y = create_sequences(df_norm, seq_len, target_col, feature_cols)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 6. Split into train, validation, and test sets\n",
    "# -------------------------------------------------------\n",
    "train_size = int(len(X) * 0.80)\n",
    "val_size   = int(len(X) * 0.10)\n",
    "\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val,   y_val   = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test,  y_test  = X[train_size+val_size:], y[train_size+val_size:]\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 7. Build and train the RNN and LSTM models\n",
    "# -------------------------------------------------------\n",
    "# Function to build a simple sequential model\n",
    "def build_model(cell_type='RNN', seq_len=seq_len, n_features=None):\n",
    "    Model = Sequential()\n",
    "    Cell = SimpleRNN if cell_type=='RNN' else LSTM\n",
    "    Model.add(Cell(40, activation='tanh', return_sequences=True, input_shape=(seq_len, n_features)))\n",
    "    Model.add(Dropout(0.15))\n",
    "    Model.add(Cell(40, activation='tanh', return_sequences=True))\n",
    "    Model.add(Dropout(0.15))\n",
    "    Model.add(Cell(40, activation='tanh'))\n",
    "    Model.add(Dropout(0.15))\n",
    "    Model.add(Dense(1))\n",
    "    Model.compile(optimizer='adam', loss='mse')\n",
    "    return Model\n",
    "\n",
    "n_features = 1 + len(feature_cols)\n",
    "\n",
    "rnn_model = build_model('RNN', seq_len, n_features)\n",
    "lstm_model = build_model('LSTM', seq_len, n_features)\n",
    "\n",
    "# Train models\n",
    "history_rnn = rnn_model.fit(X_train, y_train, epochs=50, batch_size=256, validation_data=(X_val, y_val))\n",
    "history_lstm = lstm_model.fit(X_train, y_train, epochs=50, batch_size=256, validation_data=(X_val, y_val))\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 8. Evaluate models on test set\n",
    "# -------------------------------------------------------\n",
    "def evaluate_model(model, X_test, y_test, name):\n",
    "    pred = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, pred)\n",
    "    mae = mean_absolute_error(y_test, pred)\n",
    "    mse = mean_squared_error(y_test, pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f'{name} R2 score: {r2:.4f}')\n",
    "    print(f'{name} MAE: {mae:.4f}')\n",
    "    print(f'{name} MSE: {mse:.4f}')\n",
    "    print(f'{name} RMSE: {rmse:.4f}')\n",
    "    return pred\n",
    "\n",
    "rnn_pred = evaluate_model(rnn_model, X_test, y_test, 'RNN')\n",
    "lstm_pred = evaluate_model(lstm_model, X_test, y_test, 'LSTM')\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 9. Plot predictions vs actuals\n",
    "# -------------------------------------------------------\n",
    "def plot_preds(actual, preds, labels, title):\n",
    "    plt.figure(figsize=(16,4))\n",
    "    plt.plot(actual, label='Actual', linewidth=2)\n",
    "    for pred, lbl in zip(preds, labels):\n",
    "        plt.plot(pred, alpha=0.7, label=lbl)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time steps')\n",
    "    plt.ylabel('Normalized')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_preds(\n",
    "    y_test,\n",
    "    preds=[rnn_pred, lstm_pred],\n",
    "    labels=['RNN Prediction', 'LSTM Prediction'],\n",
    "    title=f'{target_col} Predictions vs Actual'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumption = df['Total_consumption']\n",
    "consumption.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# TensorFlow imports (change to your preferred framework if needed)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, Dropout, Dense\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1. Filter data to desired date range\n",
    "# -------------------------------------------------------\n",
    "start_date = \"2022-09-11\"\n",
    "end_date   = \"2025-02-19\"\n",
    "df = df.loc[start_date:end_date]\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2. Visualize data before normalization (optional)\n",
    "# -------------------------------------------------------\n",
    "df[\"Total_consumption\"].plot(figsize=(16,4), legend=True)\n",
    "plt.title('Hourly power consumption data - BEFORE NORMALIZATION')\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3. Normalize data\n",
    "#    (Only \"Total_consumption\" in this example.)\n",
    "# -------------------------------------------------------\n",
    "def normalize_data(df):\n",
    "    scaler = MinMaxScaler()\n",
    "    df[\"Total_consumption\"] = scaler.fit_transform(\n",
    "        df[\"Total_consumption\"].values.reshape(-1,1)\n",
    "    )\n",
    "    return df, scaler\n",
    "\n",
    "df_norm, scaler = normalize_data(df.copy())\n",
    "\n",
    "# Visualize after normalization (optional)\n",
    "df_norm[\"Total_consumption\"].plot(figsize=(16,4), legend=True)\n",
    "plt.title(\"Hourly power consumption df - AFTER NORMALIZATION\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4. Create sequences from your normalized column\n",
    "# -------------------------------------------------------\n",
    "def create_sequences(data_arr, seq_len):\n",
    "    \"\"\"\n",
    "    data_arr: a 1D numpy array (already normalized)\n",
    "    seq_len : number of time steps per sample\n",
    "    Returns: X, y\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(seq_len, len(data_arr)):\n",
    "        # The previous seq_len values as input\n",
    "        X.append(data_arr[i - seq_len : i])\n",
    "        # The current value as the label\n",
    "        y.append(data_arr[i])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "seq_len = 20\n",
    "consumption_arr = df_norm[\"Total_consumption\"].values\n",
    "X, y = create_sequences(consumption_arr, seq_len)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 5. Split into train, validation, and test sets\n",
    "# -------------------------------------------------------\n",
    "# Example ratio: 70% train, 15% validation, 15% test.\n",
    "train_size = int(len(X) * 0.80)\n",
    "val_size   = int(len(X) * 0.10)\n",
    "test_size  = len(X) - train_size - val_size\n",
    "\n",
    "X_train = X[:train_size]\n",
    "y_train = y[:train_size]\n",
    "\n",
    "X_val = X[train_size : train_size + val_size]\n",
    "y_val = y[train_size : train_size + val_size]\n",
    "\n",
    "X_test = X[train_size + val_size :]\n",
    "y_test = y[train_size + val_size :]\n",
    "\n",
    "# Reshape for RNN/LSTM [samples, timesteps, features=1]\n",
    "X_train = X_train.reshape(X_train.shape[0], seq_len, 1)\n",
    "X_val   = X_val.reshape(X_val.shape[0], seq_len, 1)\n",
    "X_test  = X_test.reshape(X_test.shape[0], seq_len, 1)\n",
    "\n",
    "print(\"X_train.shape =\", X_train.shape)\n",
    "print(\"y_train.shape =\", y_train.shape)\n",
    "print(\"X_val.shape   =\", X_val.shape)\n",
    "print(\"y_val.shape   =\", y_val.shape)\n",
    "print(\"X_test.shape  =\", X_test.shape)\n",
    "print(\"y_test.shape  =\", y_test.shape)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 6. Build a simple RNN model\n",
    "# -------------------------------------------------------\n",
    "rnn_model = Sequential()\n",
    "rnn_model.add(SimpleRNN(40, activation=\"tanh\", return_sequences=True,\n",
    "                        input_shape=(seq_len, 1)))\n",
    "rnn_model.add(Dropout(0.15))\n",
    "rnn_model.add(SimpleRNN(40, activation=\"tanh\", return_sequences=True))\n",
    "rnn_model.add(Dropout(0.15))\n",
    "rnn_model.add(SimpleRNN(40, activation=\"tanh\", return_sequences=False))\n",
    "rnn_model.add(Dropout(0.15))\n",
    "rnn_model.add(Dense(1))\n",
    "\n",
    "rnn_model.summary()\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 7. Train and evaluate RNN model (with validation data)\n",
    "# -------------------------------------------------------\n",
    "rnn_model.compile(optimizer=\"adam\", loss=\"MSE\")\n",
    "\n",
    "rnn_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size=256,\n",
    "    validation_data=(X_val, y_val)  # Provide validation data\n",
    ")\n",
    "\n",
    "# RNN predictions on the test set\n",
    "rnn_predictions = rnn_model.predict(X_test)\n",
    "rnn_score = r2_score(y_test, rnn_predictions)\n",
    "print(\"R2 Score of RNN model: \", rnn_score)\n",
    "\n",
    "# Forecast errors and bias\n",
    "forecast_errors = y_test.reshape(-1) - rnn_predictions.reshape(-1)\n",
    "bias = np.mean(forecast_errors)\n",
    "print('Bias: %f' % bias)\n",
    "\n",
    "# Mean Absolute Error\n",
    "mae = mean_absolute_error(y_test, rnn_predictions)\n",
    "print('MAE: %f' % mae)\n",
    "\n",
    "# Root Mean Squared Error\n",
    "mse = mean_squared_error(y_test, rnn_predictions)\n",
    "rmse = sqrt(mse)\n",
    "print('RMSE: %f' % rmse)\n",
    "\n",
    "# Plot function\n",
    "def plot_predictions(test, predicted, title):\n",
    "    plt.figure(figsize=(16,4))\n",
    "    plt.plot(test, label='Actual power consumption')\n",
    "    plt.plot(predicted, alpha=0.7, label='Predicted power consumption')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time (test points)')\n",
    "    plt.ylabel('Normalized consumption')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_predictions(y_test, rnn_predictions, \"RNN Predictions vs Actual\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 8. Build an LSTM model (with validation data)\n",
    "# -------------------------------------------------------\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(40, activation=\"tanh\", return_sequences=True,\n",
    "                    input_shape=(seq_len, 1)))\n",
    "lstm_model.add(Dropout(0.15))\n",
    "lstm_model.add(LSTM(40, activation=\"tanh\", return_sequences=True))\n",
    "lstm_model.add(Dropout(0.15))\n",
    "lstm_model.add(LSTM(40, activation=\"tanh\", return_sequences=False))\n",
    "lstm_model.add(Dropout(0.15))\n",
    "lstm_model.add(Dense(1))\n",
    "\n",
    "lstm_model.summary()\n",
    "\n",
    "lstm_model.compile(optimizer=\"adam\", loss=\"MSE\")\n",
    "\n",
    "lstm_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size=256,\n",
    "    validation_data=(X_val, y_val)  # Provide validation data\n",
    ")\n",
    "\n",
    "# LSTM predictions on the test set\n",
    "lstm_predictions = lstm_model.predict(X_test)\n",
    "lstm_score = r2_score(y_test, lstm_predictions)\n",
    "print(\"R^2 Score of LSTM model:\", lstm_score)\n",
    "\n",
    "# Forecast errors and bias\n",
    "forecast_errors_lstm = y_test.reshape(-1) - lstm_predictions.reshape(-1)\n",
    "bias_lstm = np.mean(forecast_errors_lstm)\n",
    "print('Bias (LSTM): %f' % bias_lstm)\n",
    "\n",
    "# Mean Absolute Error\n",
    "mae_lstm = mean_absolute_error(y_test, lstm_predictions)\n",
    "print('MAE (LSTM): %f' % mae_lstm)\n",
    "\n",
    "# Root Mean Squared Error\n",
    "mse_lstm = mean_squared_error(y_test, lstm_predictions)\n",
    "rmse_lstm = sqrt(mse_lstm)\n",
    "print('RMSE (LSTM): %f' % rmse_lstm)\n",
    "\n",
    "plot_predictions(y_test, lstm_predictions, \"LSTM Predictions vs Actual\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 9. Compare RNN vs LSTM predictions in one graph\n",
    "# -------------------------------------------------------\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.plot(y_test, label=\"Actual values\", linewidth=3)\n",
    "plt.plot(rnn_predictions, label=\"RNN predictions\", linewidth=3, alpha=0.7)\n",
    "plt.plot(lstm_predictions, label=\"LSTM predictions\", linewidth=3, alpha=0.7)\n",
    "plt.legend()\n",
    "plt.title(\"Predictions vs Actual Data\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# TensorFlow imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, Dropout, Dense\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1. Filter data to desired date range\n",
    "# -------------------------------------------------------\n",
    "start_date = \"2022-09-11\"\n",
    "end_date   = \"2025-02-19\"\n",
    "df = df.loc[start_date:end_date]\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2. (Optional) Visualize data before normalization\n",
    "# -------------------------------------------------------\n",
    "df[\"Total_consumption\"].plot(figsize=(16,4), legend=True)\n",
    "plt.title('Hourly power consumption data - BEFORE NORMALIZATION')\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3. Normalize data\n",
    "# -------------------------------------------------------\n",
    "def normalize_data(df):\n",
    "    scaler = MinMaxScaler()\n",
    "    df[\"Total_consumption\"] = scaler.fit_transform(\n",
    "        df[\"Total_consumption\"].values.reshape(-1,1)\n",
    "    )\n",
    "    return df, scaler\n",
    "\n",
    "df_norm, scaler = normalize_data(df.copy())\n",
    "\n",
    "# (Optional) Visualize after normalization\n",
    "df_norm[\"Total_consumption\"].plot(figsize=(16,4), legend=True)\n",
    "plt.title(\"Hourly power consumption data - AFTER NORMALIZATION\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4. Create sequences for multi-step forecasting\n",
    "#    We want each sample to predict the next 24 hours.\n",
    "# -------------------------------------------------------\n",
    "def create_sequences(data_arr, seq_len, forecast_horizon):\n",
    "    \"\"\"\n",
    "    data_arr: 1D NumPy array (already normalized).\n",
    "    seq_len: Number of historical time steps per sample.\n",
    "    forecast_horizon: How many hours ahead we want to forecast.\n",
    "    \n",
    "    Returns:\n",
    "      X: shape (num_samples, seq_len)\n",
    "      y: shape (num_samples, forecast_horizon)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(seq_len, len(data_arr) - forecast_horizon + 1):\n",
    "        # The previous seq_len values as input\n",
    "        X.append(data_arr[i - seq_len : i])\n",
    "        # The next 'forecast_horizon' values as the label\n",
    "        y.append(data_arr[i : i + forecast_horizon])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "seq_len = 20\n",
    "forecast_horizon = 24\n",
    "\n",
    "consumption_arr = df_norm[\"Total_consumption\"].values\n",
    "\n",
    "# Create multi-step sequences\n",
    "X, y = create_sequences(consumption_arr, seq_len, forecast_horizon)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 5. Split into train, validation, and test sets\n",
    "# -------------------------------------------------------\n",
    "train_size = int(len(X) * 0.80)\n",
    "val_size   = int(len(X) * 0.10)\n",
    "test_size  = len(X) - train_size - val_size\n",
    "\n",
    "X_train = X[:train_size]\n",
    "y_train = y[:train_size]\n",
    "\n",
    "X_val = X[train_size : train_size + val_size]\n",
    "y_val = y[train_size : train_size + val_size]\n",
    "\n",
    "X_test = X[train_size + val_size :]\n",
    "y_test = y[train_size + val_size :]\n",
    "\n",
    "# Reshape for RNN/LSTM: [samples, timesteps, features=1]\n",
    "X_train = X_train.reshape(X_train.shape[0], seq_len, 1)\n",
    "X_val   = X_val.reshape(X_val.shape[0], seq_len, 1)\n",
    "X_test  = X_test.reshape(X_test.shape[0], seq_len, 1)\n",
    "\n",
    "print(\"X_train.shape =\", X_train.shape)  # (num_train_samples, 20, 1)\n",
    "print(\"y_train.shape =\", y_train.shape)  # (num_train_samples, 24)\n",
    "print(\"X_val.shape   =\", X_val.shape)\n",
    "print(\"y_val.shape   =\", y_val.shape)\n",
    "print(\"X_test.shape  =\", X_test.shape)\n",
    "print(\"y_test.shape  =\", y_test.shape)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 6. Build an RNN model for 24-hour forecasting\n",
    "# -------------------------------------------------------\n",
    "rnn_model = Sequential()\n",
    "rnn_model.add(SimpleRNN(40, activation=\"tanh\", return_sequences=True,\n",
    "                        input_shape=(seq_len, 1)))\n",
    "rnn_model.add(Dropout(0.15))\n",
    "rnn_model.add(SimpleRNN(40, activation=\"tanh\", return_sequences=True))\n",
    "rnn_model.add(Dropout(0.15))\n",
    "rnn_model.add(SimpleRNN(40, activation=\"tanh\", return_sequences=False))\n",
    "rnn_model.add(Dropout(0.15))\n",
    "\n",
    "# Output Dense now has 'forecast_horizon' neurons (24)\n",
    "rnn_model.add(Dense(forecast_horizon))\n",
    "\n",
    "rnn_model.summary()\n",
    "\n",
    "rnn_model.compile(optimizer=\"adam\", loss=\"MSE\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 7. Train RNN model\n",
    "# -------------------------------------------------------\n",
    "rnn_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size=256,\n",
    "    validation_data=(X_val, y_val)\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 8. Evaluate the RNN on the test set\n",
    "# -------------------------------------------------------\n",
    "rnn_predictions = rnn_model.predict(X_test)\n",
    "# rnn_predictions.shape => (num_test_samples, 24)\n",
    "\n",
    "# Flatten actual & predictions so we can compute overall metrics\n",
    "rnn_predictions_flat = rnn_predictions.flatten()\n",
    "y_test_flat = y_test.flatten()\n",
    "\n",
    "rnn_score = r2_score(y_test_flat, rnn_predictions_flat)\n",
    "mae = mean_absolute_error(y_test_flat, rnn_predictions_flat)\n",
    "mse = mean_squared_error(y_test_flat, rnn_predictions_flat)\n",
    "rmse = sqrt(mse)\n",
    "bias = np.mean(y_test_flat - rnn_predictions_flat)\n",
    "\n",
    "print(\"R^2 Score (RNN): \", rnn_score)\n",
    "print('MAE (RNN): ', mae)\n",
    "print('RMSE (RNN): ', rmse)\n",
    "print('Bias (RNN): ', bias)\n",
    "\n",
    "# (Optional) Plot a single example of 24-hour predictions\n",
    "def plot_24h_forecast(sample_idx):\n",
    "    \"\"\"\n",
    "    Plots the actual vs predicted for one sample in the test set.\n",
    "    sample_idx is an index into X_test / y_test.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    # Actual 24-hour values\n",
    "    plt.plot(y_test[sample_idx], label='Actual Next 24h')\n",
    "    # Predicted 24-hour values\n",
    "    plt.plot(rnn_predictions[sample_idx], label='Predicted Next 24h')\n",
    "    plt.title(f'RNN 24-hour Forecast - Sample index {sample_idx}')\n",
    "    plt.xlabel('Hour Ahead')\n",
    "    plt.ylabel('Normalized consumption')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example: plot predictions for the first (or last) test sample\n",
    "plot_24h_forecast(sample_idx=0)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 9. Build and train an LSTM model for 24-hour forecasting\n",
    "# -------------------------------------------------------\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(40, activation=\"tanh\", return_sequences=True,\n",
    "                    input_shape=(seq_len, 1)))\n",
    "lstm_model.add(Dropout(0.15))\n",
    "lstm_model.add(LSTM(40, activation=\"tanh\", return_sequences=True))\n",
    "lstm_model.add(Dropout(0.15))\n",
    "lstm_model.add(LSTM(40, activation=\"tanh\", return_sequences=False))\n",
    "lstm_model.add(Dropout(0.15))\n",
    "lstm_model.add(Dense(forecast_horizon))\n",
    "\n",
    "lstm_model.summary()\n",
    "lstm_model.compile(optimizer=\"adam\", loss=\"MSE\")\n",
    "\n",
    "lstm_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size=256,\n",
    "    validation_data=(X_val, y_val)\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 10. Evaluate the LSTM model on the test set\n",
    "# -------------------------------------------------------\n",
    "lstm_predictions = lstm_model.predict(X_test)\n",
    "# Flatten for overall metrics\n",
    "lstm_predictions_flat = lstm_predictions.flatten()\n",
    "y_test_flat = y_test.flatten()\n",
    "\n",
    "lstm_score = r2_score(y_test_flat, lstm_predictions_flat)\n",
    "mae_lstm = mean_absolute_error(y_test_flat, lstm_predictions_flat)\n",
    "mse_lstm = mean_squared_error(y_test_flat, lstm_predictions_flat)\n",
    "rmse_lstm = sqrt(mse_lstm)\n",
    "bias_lstm = np.mean(y_test_flat - lstm_predictions_flat)\n",
    "\n",
    "print(\"R^2 Score (LSTM):\", lstm_score)\n",
    "print('MAE (LSTM):', mae_lstm)\n",
    "print('RMSE (LSTM):', rmse_lstm)\n",
    "print('Bias (LSTM):', bias_lstm)\n",
    "\n",
    "# Plot one example of 24-hour LSTM forecasts\n",
    "def plot_24h_forecast_lstm(sample_idx):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(y_test[sample_idx], label='Actual Next 24h')\n",
    "    plt.plot(lstm_predictions[sample_idx], label='Predicted Next 24h')\n",
    "    plt.title(f'LSTM 24-hour Forecast - Sample index {sample_idx}')\n",
    "    plt.xlabel('Hour Ahead')\n",
    "    plt.ylabel('Normalized consumption')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_24h_forecast_lstm(sample_idx=0)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 11. Compare RNN vs LSTM for a single test sample\n",
    "# -------------------------------------------------------\n",
    "sample_idx = 0\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(y_test[sample_idx], label=\"Actual 24h\", linewidth=3)\n",
    "plt.plot(rnn_predictions[sample_idx], label=\"RNN\", linewidth=3, alpha=0.7)\n",
    "plt.plot(lstm_predictions[sample_idx], label=\"LSTM\", linewidth=3, alpha=0.7)\n",
    "plt.legend()\n",
    "plt.title(f\"24-hour Forecast Comparison - Sample {sample_idx}\")\n",
    "plt.xlabel('Hour Ahead')\n",
    "plt.ylabel('Normalized consumption')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# TensorFlow imports (change to your preferred framework if needed)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, Dropout, Dense\n",
    "\n",
    "# Filter data to your chosen date range\n",
    "start_date = \"2022-09-11\"\n",
    "end_date   = \"2025-02-19\"\n",
    "df = df.loc[start_date:end_date]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 2. Additional features (including cyclical and categorical embeddings)\n",
    "# ------------------------------------------------------------------------\n",
    "# Example holiday set (Belgian holidays, or any other). \n",
    "# Replace with actual holiday dates as a set of datetime.date objects:\n",
    "be_holidays = {\n",
    "    # For illustration, pretend we have a few random holiday dates\n",
    "    # e.g., datetime(2022, 12, 25).date()\n",
    "    # or use whatever holiday set your project requires\n",
    "}\n",
    "\n",
    "# Hour, day-of-week, month\n",
    "df['hour'] = df.index.hour\n",
    "df['day_of_week'] = df.index.dayofweek\n",
    "df['month'] = df.index.month\n",
    "\n",
    "# Categorical features\n",
    "df['is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "df['is_festive'] = df.index.to_series().apply(lambda x: 1 if x.date() in be_holidays else 0)\n",
    "df['working_hour'] = df['hour'].apply(lambda x: 1 if 8 <= x <= 18 else 0)\n",
    "\n",
    "# Seasonal features\n",
    "df['is_summer'] = df.index.month.isin([6, 7, 8]).astype(int)\n",
    "df['is_winter'] = df.index.month.isin([12, 1, 2]).astype(int)\n",
    "\n",
    "# Peak hour features\n",
    "df['is_morning_peak'] = ((df['hour'] >= 7) & (df['hour'] <= 9)).astype(int)\n",
    "df['is_evening_peak'] = ((df['hour'] >= 17) & (df['hour'] <= 20)).astype(int)\n",
    "\n",
    "# Cyclical features for hour and day of week\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 3. Inspect the DataFrame\n",
    "# ------------------------------------------------------------------------\n",
    "print(\"DataFrame after feature engineering:\")\n",
    "print(df.head())\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 4. Normalize the data\n",
    "#    We'll use one MinMaxScaler for all columns used as inputs (including\n",
    "#    'Total_consumption') so everything is on [0,1].\n",
    "# ------------------------------------------------------------------------\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(df),\n",
    "    columns=df.columns,\n",
    "    index=df.index\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 5. Create sequences for a multi-feature time series\n",
    "# ------------------------------------------------------------------------\n",
    "def create_sequences_multifeature(df, seq_len, target_col):\n",
    "    \"\"\"\n",
    "    df: DataFrame of scaled features (all numeric).\n",
    "    seq_len: number of time steps per sequence.\n",
    "    target_col: name of the target column to predict (e.g. 'Total_consumption').\n",
    "    \n",
    "    Returns:\n",
    "    X - shape: (num_samples, seq_len, num_features)\n",
    "    y - shape: (num_samples,)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    data_values = df.values  # shape: (num_samples, num_features)\n",
    "    target_idx = df.columns.get_loc(target_col)\n",
    "    \n",
    "    for i in range(seq_len, len(df)):\n",
    "        # For each sequence, we take rows [i-seq_len, i) of all columns\n",
    "        X.append(data_values[i-seq_len:i, :])\n",
    "        # The label is the target column at position i\n",
    "        y.append(data_values[i, target_idx])\n",
    "        \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "# We'll predict \"Total_consumption\"\n",
    "seq_len = 20\n",
    "target_col = \"Total_consumption\"\n",
    "X, y = create_sequences_multifeature(df_scaled, seq_len, target_col)\n",
    "\n",
    "print(f\"\\nCreated sequences with shape X: {X.shape}, y: {y.shape}\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 6. Split into train/test sets\n",
    "# ------------------------------------------------------------------------\n",
    "train_size = int(len(X) * 0.8)  # 80% train, 20% test\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "print(\"X_train.shape =\", X_train.shape)  # (train_size, seq_len, n_features)\n",
    "print(\"y_train.shape =\", y_train.shape)\n",
    "print(\"X_test.shape =\", X_test.shape)    # (test_size, seq_len, n_features)\n",
    "print(\"y_test.shape =\", y_test.shape)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 7. Build and train a Simple RNN model\n",
    "# ------------------------------------------------------------------------\n",
    "rnn_model = Sequential()\n",
    "rnn_model.add(SimpleRNN(40, activation=\"tanh\", return_sequences=True,\n",
    "                        input_shape=(seq_len, X.shape[2])))\n",
    "rnn_model.add(Dropout(0.15))\n",
    "rnn_model.add(SimpleRNN(40, activation=\"tanh\", return_sequences=True))\n",
    "rnn_model.add(Dropout(0.15))\n",
    "rnn_model.add(SimpleRNN(40, activation=\"tanh\", return_sequences=False))\n",
    "rnn_model.add(Dropout(0.15))\n",
    "rnn_model.add(Dense(1))\n",
    "\n",
    "rnn_model.summary()\n",
    "\n",
    "rnn_model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "rnn_model.fit(X_train, y_train, epochs=50, batch_size=256)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 8. Evaluate RNN\n",
    "# ------------------------------------------------------------------------\n",
    "rnn_predictions = rnn_model.predict(X_test)\n",
    "rnn_score = r2_score(y_test, rnn_predictions)\n",
    "print(\"R2 Score (RNN):\", rnn_score)\n",
    "\n",
    "mse_rnn = mean_squared_error(y_test, rnn_predictions)\n",
    "rmse_rnn = sqrt(mse_rnn)\n",
    "mae_rnn = mean_absolute_error(y_test, rnn_predictions)\n",
    "print(\"MSE (RNN):\", mse_rnn)\n",
    "print(\"RMSE (RNN):\", rmse_rnn)\n",
    "print(\"MAE (RNN):\", mae_rnn)\n",
    "\n",
    "# Simple plotting function\n",
    "def plot_predictions(test, predicted, title):\n",
    "    plt.figure(figsize=(16,4))\n",
    "    plt.plot(test, label='Actual')\n",
    "    plt.plot(predicted, alpha=0.7, label='Predicted')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Test Sample')\n",
    "    plt.ylabel('Scaled Consumption')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_predictions(y_test, rnn_predictions, \"RNN Predictions vs Actual\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 9. Build and train an LSTM model\n",
    "# ------------------------------------------------------------------------\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(40, activation=\"tanh\", return_sequences=True, \n",
    "                    input_shape=(seq_len, X.shape[2])))\n",
    "lstm_model.add(Dropout(0.15))\n",
    "lstm_model.add(LSTM(40, activation=\"tanh\", return_sequences=True))\n",
    "lstm_model.add(Dropout(0.15))\n",
    "lstm_model.add(LSTM(40, activation=\"tanh\", return_sequences=False))\n",
    "lstm_model.add(Dropout(0.15))\n",
    "lstm_model.add(Dense(1))\n",
    "\n",
    "lstm_model.summary()\n",
    "\n",
    "lstm_model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "lstm_model.fit(X_train, y_train, epochs=50, batch_size=256)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 10. Evaluate LSTM\n",
    "# ------------------------------------------------------------------------\n",
    "lstm_predictions = lstm_model.predict(X_test)\n",
    "lstm_score = r2_score(y_test, lstm_predictions)\n",
    "print(\"R2 Score (LSTM):\", lstm_score)\n",
    "\n",
    "mse_lstm = mean_squared_error(y_test, lstm_predictions)\n",
    "rmse_lstm = sqrt(mse_lstm)\n",
    "mae_lstm = mean_absolute_error(y_test, lstm_predictions)\n",
    "print(\"MSE (LSTM):\", mse_lstm)\n",
    "print(\"RMSE (LSTM):\", rmse_lstm)\n",
    "print(\"MAE (LSTM):\", mae_lstm)\n",
    "\n",
    "plot_predictions(y_test, lstm_predictions, \"LSTM Predictions vs Actual\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 11. Compare RNN vs LSTM in a single plot\n",
    "# ------------------------------------------------------------------------\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(y_test, label=\"Actual\", linewidth=3)\n",
    "plt.plot(rnn_predictions, label=\"RNN predictions\", linewidth=3, alpha=0.7)\n",
    "plt.plot(lstm_predictions, label=\"LSTM predictions\", linewidth=3, alpha=0.7)\n",
    "plt.title(\"Predictions vs Actual Data\", fontsize=16)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# --- 1. Preprocess (SAME as before, not repeated here) ---\n",
    "# Assume `df` is already filtered, feature-engineered, and scaled\n",
    "\n",
    "# --- 2. Sequence Creation (again, same as before) ---\n",
    "seq_len = 72\n",
    "target_col = \"Total_consumption\"\n",
    "X, y = create_sequences_multifeature(df_scaled, seq_len, target_col)\n",
    "\n",
    "# --- 3. Split Train/Test ---\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# --- 4. Define custom loss ---\n",
    "def weighted_mse(y_true, y_pred):\n",
    "    weight = K.abs(y_true - K.mean(y_true))\n",
    "    return K.mean(weight * K.square(y_pred - y_true))\n",
    "\n",
    "# --- 5. First model: baseline LSTM ---\n",
    "base_model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(seq_len, X.shape[2]), activation='tanh'),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64, return_sequences=True, activation='tanh'),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64, activation='tanh'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)\n",
    "])\n",
    "base_model.compile(optimizer='adam', loss=weighted_mse)\n",
    "base_model.fit(X_train, y_train, epochs=50, batch_size=128, validation_split=0.1, verbose=1)\n",
    "\n",
    "# --- 6. Predict and get residuals ---\n",
    "base_pred_train = base_model.predict(X_train).flatten()\n",
    "base_pred_test = base_model.predict(X_test).flatten()\n",
    "\n",
    "residuals_train = y_train - base_pred_train\n",
    "residuals_test = y_test - base_pred_test  # for evaluation only\n",
    "\n",
    "# --- 7. Train second model on residuals ---\n",
    "residual_model = Sequential([\n",
    "    LSTM(32, return_sequences=True, input_shape=(seq_len, X.shape[2]), activation='tanh'),\n",
    "    Dropout(0.2),\n",
    "    LSTM(32, activation='tanh'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)\n",
    "])\n",
    "residual_model.compile(optimizer='adam', loss='mse')\n",
    "residual_model.fit(X_train, residuals_train, epochs=30, batch_size=128, validation_split=0.1, verbose=1)\n",
    "\n",
    "# --- 8. Predict residuals and combine ---\n",
    "residual_pred_test = residual_model.predict(X_test).flatten()\n",
    "final_predictions = base_pred_test + residual_pred_test\n",
    "\n",
    "# --- 9. Evaluation ---\n",
    "r2 = r2_score(y_test, final_predictions)\n",
    "mse = mean_squared_error(y_test, final_predictions)\n",
    "rmse = sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, final_predictions)\n",
    "\n",
    "print(\"\\n🧠 Residual-Boosted Forecast Results:\")\n",
    "print(f\"R2 Score: {r2}\")\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "\n",
    "# --- 10. Plot ---\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(y_test, label=\"Actual\", linewidth=2)\n",
    "plt.plot(base_pred_test, label=\"Base LSTM\", linewidth=2, alpha=0.6)\n",
    "plt.plot(final_predictions, label=\"Residual-Boosted\", linewidth=2, alpha=0.9)\n",
    "plt.title(\"Hybrid Model: Base + Residual Forecasting\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "\n",
    "# -------------------- 1. Preprocessing --------------------\n",
    "# Filter date range\n",
    "start_date = \"2022-09-11\"\n",
    "end_date   = \"2025-02-19\"\n",
    "df = df.loc[start_date:end_date]\n",
    "\n",
    "# Holidays placeholder\n",
    "be_holidays = set()\n",
    "\n",
    "# Time-based features\n",
    "df['hour'] = df.index.hour\n",
    "df['day_of_week'] = df.index.dayofweek\n",
    "df['month'] = df.index.month\n",
    "df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "df['is_festive'] = df.index.to_series().apply(lambda x: 1 if x.date() in be_holidays else 0)\n",
    "df['working_hour'] = df['hour'].apply(lambda x: 1 if 8 <= x <= 18 else 0)\n",
    "df['is_summer'] = df['month'].isin([6, 7, 8]).astype(int)\n",
    "df['is_winter'] = df['month'].isin([12, 1, 2]).astype(int)\n",
    "\n",
    "# Cyclical encodings\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "\n",
    "# Rolling peak/min features\n",
    "#df['past_12h_max'] = df['Total_consumption'].rolling(window=12).max().shift(1).bfill()\n",
    "#df['past_12h_min'] = df['Total_consumption'].rolling(window=12).min().shift(1).bfill()\n",
    "\n",
    "# -------------------- 2. Normalization --------------------\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index)\n",
    "\n",
    "# -------------------- 3. Create multi-step sequences --------------------\n",
    "def create_multi_step_sequences(df, seq_len, target_col, forecast_horizon):\n",
    "    X, y = [], []\n",
    "    data = df.values\n",
    "    target_idx = df.columns.get_loc(target_col)\n",
    "    for i in range(seq_len, len(df) - forecast_horizon):\n",
    "        X.append(data[i-seq_len:i])\n",
    "        y.append(data[i:i+forecast_horizon, target_idx])  # next 24 hours\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "seq_len =168  # Past 72 hours\n",
    "forecast_horizon = 24  # Predict next 24 hours\n",
    "target_col = \"Total_consumption\"\n",
    "\n",
    "X, y = create_multi_step_sequences(df_scaled, seq_len, target_col, forecast_horizon)\n",
    "\n",
    "# -------------------- 4. Train/test split --------------------\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# -------------------- 5. Define LSTM model --------------------\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(seq_len, X.shape[2]), activation='tanh'),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64, return_sequences=True, activation='tanh'),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64, activation='tanh'),\n",
    "    Dropout(0.2),\n",
    "    Dense(forecast_horizon)  # Output vector of 24 predictions\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()\n",
    "\n",
    "# -------------------- 6. Train model --------------------\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# -------------------- 7. Predict --------------------\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# -------------------- 8. Evaluation --------------------\n",
    "r2 = r2_score(y_test.flatten(), predictions.flatten())\n",
    "mse = mean_squared_error(y_test.flatten(), predictions.flatten())\n",
    "rmse = sqrt(mse)\n",
    "mae = mean_absolute_error(y_test.flatten(), predictions.flatten())\n",
    "\n",
    "print(\"\\n📊 Multi-step Forecast Evaluation (24h ahead):\")\n",
    "print(f\"R2 Score: {r2}\")\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "\n",
    "# -------------------- 9. Plot sample predictions --------------------\n",
    "for i in range(3):\n",
    "    plt.figure(figsize=(12,3))\n",
    "    plt.plot(y_test[i], label=\"Actual (24h)\", marker='o')\n",
    "    plt.plot(predictions[i], label=\"Predicted (24h)\", marker='x')\n",
    "    plt.title(f\"24h Forecast - Sample {i+1}\")\n",
    "    plt.xlabel(\"Hour Ahead\")\n",
    "    plt.ylabel(\"Scaled Consumption\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "\n",
    "# -------------------- 1. Preprocessing --------------------\n",
    "# Define date range. We now extend end_date by 1 day to include 2025-02-20 23:00, \n",
    "# so that our DataFrame has rows for Feb 20 (though consumption might be NaN).\n",
    "start_date = \"2022-09-11\"\n",
    "end_date   = \"2025-02-20 23:00\"\n",
    "df = df.loc[start_date:end_date]\n",
    "\n",
    "# If you do NOT have actual consumption for 2025-02-20, \n",
    "# you can fill it with NaN or 0.  For example:\n",
    "# df['Total_consumption'] = df['Total_consumption'].fillna(0)\n",
    "\n",
    "# Add time-based features\n",
    "be_holidays = set()\n",
    "df['hour'] = df.index.hour\n",
    "df['day_of_week'] = df.index.dayofweek\n",
    "df['month'] = df.index.month\n",
    "df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "df['is_festive'] = df.index.to_series().apply(lambda x: 1 if x.date() in be_holidays else 0)\n",
    "df['working_hour'] = df['hour'].apply(lambda x: 1 if 8 <= x <= 18 else 0)\n",
    "df['is_summer'] = df['month'].isin([6, 7, 8]).astype(int)\n",
    "df['is_winter'] = df['month'].isin([12, 1, 2]).astype(int)\n",
    "\n",
    "# Cyclical time encodings\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "\n",
    "# -------------------- 2. Normalization --------------------\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), \n",
    "                         columns=df.columns, \n",
    "                         index=df.index)\n",
    "\n",
    "# -------------------- 3. Create multi-step sequences --------------------\n",
    "def create_multi_step_sequences(df, seq_len, target_col, forecast_horizon):\n",
    "    X, y = [], []\n",
    "    data = df.values\n",
    "    target_idx = df.columns.get_loc(target_col)\n",
    "    for i in range(seq_len, len(df) - forecast_horizon):\n",
    "        X.append(data[i-seq_len:i])\n",
    "        y.append(data[i:i+forecast_horizon, target_idx])  # next 24 hours\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "seq_len = 168           # Use past 168 hours\n",
    "forecast_horizon = 24   # Predict next 24 hours\n",
    "target_col = \"Total_consumption\"\n",
    "\n",
    "X, y = create_multi_step_sequences(df_scaled, seq_len, target_col, forecast_horizon)\n",
    "\n",
    "# -------------------- 4. Train/test split --------------------\n",
    "# We'll do a simple split on the entire scaled dataset.\n",
    "# Make sure the final day (Feb 20) is in the test set so that\n",
    "# we can produce a forecast for it.\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# -------------------- 5. Define LSTM model --------------------\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(seq_len, X.shape[2]), activation='tanh'),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64, return_sequences=True, activation='tanh'),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64, activation='tanh'),\n",
    "    Dropout(0.2),\n",
    "    Dense(forecast_horizon)  # Output vector of 24 predictions\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()\n",
    "\n",
    "# -------------------- 6. Train model --------------------\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# -------------------- 7. Predict on the test set (for evaluation) --------------------\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# -------------------- 8. Evaluation --------------------\n",
    "r2 = r2_score(y_test.flatten(), predictions.flatten())\n",
    "mse = mean_squared_error(y_test.flatten(), predictions.flatten())\n",
    "rmse = sqrt(mse)\n",
    "mae = mean_absolute_error(y_test.flatten(), predictions.flatten())\n",
    "\n",
    "print(\"\\n📊 Multi-step Forecast Evaluation (24h ahead) on test set:\")\n",
    "print(f\"R2 Score: {r2}\")\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "\n",
    "# -------------------- 9. Plot some sample predictions --------------------\n",
    "for i in range(3):\n",
    "    plt.figure(figsize=(12,3))\n",
    "    plt.plot(y_test[i], label=\"Actual (24h)\", marker='o')\n",
    "    plt.plot(predictions[i], label=\"Predicted (24h)\", marker='x')\n",
    "    plt.title(f\"24h Forecast - Sample {i+1}\")\n",
    "    plt.xlabel(\"Hour Ahead\")\n",
    "    plt.ylabel(\"Scaled Consumption\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# -------------------- 10. Produce final 24h forecast for 20/02/2025 --------------------\n",
    "# We want the sequence that ends on 2025-02-19 23:00 so we can predict \n",
    "# from 2025-02-20 00:00 to 2025-02-20 23:00.\n",
    "\n",
    "# 1) Identify the *index* of the last valid sequence that will forecast into 20/02.\n",
    "#    This corresponds to the final place in df_scaled where the next 24 hours \n",
    "#    is exactly 2025-02-20's data.\n",
    "\n",
    "# Because we built X, y in create_multi_step_sequences from the entire df_scaled,\n",
    "# we can simply take the last item of X if we want the last 24-hour forecast in the dataset:\n",
    "X_for_20feb = X[-1:]  # shape becomes (1, 168, num_features)\n",
    "\n",
    "# 2) Predict scaled\n",
    "pred_24h_scaled = model.predict(X_for_20feb)[0]  # shape (24,)\n",
    "\n",
    "# 3) Inverse-scale only the target column\n",
    "#    We'll construct a \"dummy\" array with the same number of columns as df, \n",
    "#    place our scaled predictions in the target column, and run inverse_transform.\n",
    "dummy = np.zeros((forecast_horizon, df_scaled.shape[1]))\n",
    "target_idx = df_scaled.columns.get_loc(target_col)\n",
    "dummy[:, target_idx] = pred_24h_scaled\n",
    "dummy_inversed = scaler.inverse_transform(dummy)\n",
    "\n",
    "# Extract just the \"Total_consumption\" column\n",
    "forecast_20feb = dummy_inversed[:, target_idx]\n",
    "\n",
    "# 4) Build a timestamp index for the 24 forecast hours (20/02/2025 00:00 to 23:00)\n",
    "#    NOTE: Because our final sequence in create_multi_step_sequences was presumably \n",
    "#          the chunk that ends right before 20/02, these predicted 24 hours \n",
    "#          align with 20/02/2025 00:00..23:00\n",
    "ts_index_20feb = pd.date_range(start=\"2025-02-20 00:00:00\", periods=24, freq='H')\n",
    "forecast_df = pd.DataFrame({\"Predicted_Consumption\": forecast_20feb}, index=ts_index_20feb)\n",
    "\n",
    "print(\"\\nFinal Forecast for 20/02/2025:\")\n",
    "print(forecast_df)\n",
    "\n",
    "# (Optional) Plot the final forecast\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.plot(forecast_df.index, forecast_20feb, marker='o', label=\"Forecast Feb 20\")\n",
    "plt.title(\"24h Total_consumption Consumption Forecast on 20/02/2025\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Consumption\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# For saving the scaler\n",
    "from joblib import dump\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 1) Load data (already in a DataFrame 'df' with a DateTimeIndex).\n",
    "#    For demonstration, let's assume columns are: ['Total_consumption'].\n",
    "#    Sort by index to ensure chronological order, then split.\n",
    "# ----------------------------------------------------------\n",
    "df = df.sort_index()\n",
    "\n",
    "total_samples = len(df)\n",
    "train_size = int(total_samples * 0.7)\n",
    "val_size = int(total_samples * 0.1)  # 70% train, 10% val, 20% test\n",
    "test_size = total_samples - train_size - val_size\n",
    "\n",
    "train_df = df.iloc[:train_size].copy()\n",
    "val_df   = df.iloc[train_size:train_size+val_size].copy()\n",
    "test_df  = df.iloc[train_size+val_size:].copy()\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 2) Feature Engineering\n",
    "#    We'll create time-based columns in each set separately\n",
    "#    so we don't leak future info.\n",
    "# ----------------------------------------------------------\n",
    "# Example: hour, day_of_week. \n",
    "train_df['hour'] = train_df.index.hour\n",
    "train_df['day_of_week'] = train_df.index.dayofweek\n",
    "\n",
    "val_df['hour'] = val_df.index.hour\n",
    "val_df['day_of_week'] = val_df.index.dayofweek\n",
    "\n",
    "test_df['hour'] = test_df.index.hour\n",
    "test_df['day_of_week'] = test_df.index.dayofweek\n",
    "\n",
    "# Example lags/rolling. Adjust if day-ahead vs next hour:\n",
    "train_df['lag1'] = train_df['Total_consumption'].shift(1)\n",
    "train_df['rolling24'] = train_df['Total_consumption'].shift(1).rolling(window=24, min_periods=1).mean()\n",
    "\n",
    "val_df['lag1'] = val_df['Total_consumption'].shift(1)\n",
    "val_df['rolling24'] = val_df['Total_consumption'].shift(1).rolling(window=24, min_periods=1).mean()\n",
    "\n",
    "test_df['lag1'] = test_df['Total_consumption'].shift(1)\n",
    "test_df['rolling24'] = test_df['Total_consumption'].shift(1).rolling(window=24, min_periods=1).mean()\n",
    "\n",
    "# Drop NA (start of each set might have NaN from shifting)\n",
    "train_df.dropna(inplace=True)\n",
    "val_df.dropna(inplace=True)\n",
    "test_df.dropna(inplace=True)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 3) Scaling\n",
    "#    Fit on train only, transform val/test.\n",
    "# ----------------------------------------------------------\n",
    "scaler = MinMaxScaler()\n",
    "features_to_scale = ['Total_consumption','lag1','rolling24']\n",
    "\n",
    "# Fit on training data\n",
    "train_df[features_to_scale] = scaler.fit_transform(train_df[features_to_scale])\n",
    "# Save the fitted scaler for future reuse\n",
    "dump(scaler, './models/total_consumption_scaler.joblib')\n",
    "print(\"Scaler fitted and saved to './models/total_consumption_scaler.joblib'\")\n",
    "\n",
    "# Now transform validation and test sets (using the same scaler)\n",
    "val_df[features_to_scale]   = scaler.transform(val_df[features_to_scale])\n",
    "test_df[features_to_scale]  = scaler.transform(test_df[features_to_scale])\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 4) Create sequences for LSTM\n",
    "#    We'll store day_of_week for the target hour to group error analysis later.\n",
    "# ----------------------------------------------------------\n",
    "sequence_length = 24\n",
    "feature_cols = ['Total_consumption','hour','day_of_week','lag1','rolling24']\n",
    "target_col = 'Total_consumption'\n",
    "\n",
    "def make_sequences(df, feature_cols, target_col, seq_len):\n",
    "    X, y = [], []\n",
    "    day_of_week_target = []\n",
    "    \n",
    "    values = df[feature_cols].values\n",
    "    targets = df[target_col].values\n",
    "    dow_values = df['day_of_week'].values  # for boxplot grouping\n",
    "    \n",
    "    for i in range(seq_len, len(df)):\n",
    "        X.append(values[i-seq_len:i])\n",
    "        y.append(targets[i])\n",
    "        # The day_of_week for the target time step i:\n",
    "        day_of_week_target.append(dow_values[i])\n",
    "        \n",
    "    return np.array(X), np.array(y), np.array(day_of_week_target)\n",
    "\n",
    "X_train, y_train, dow_train = make_sequences(train_df, feature_cols, target_col, sequence_length)\n",
    "X_val,   y_val,   dow_val   = make_sequences(val_df,   feature_cols, target_col, sequence_length)\n",
    "X_test,  y_test,  dow_test  = make_sequences(test_df,  feature_cols, target_col, sequence_length)\n",
    "\n",
    "print(\"Train shapes:\", X_train.shape, y_train.shape)\n",
    "print(\"Val shapes:  \", X_val.shape,   y_val.shape)\n",
    "print(\"Test shapes: \", X_test.shape,  y_test.shape)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 5) Build LSTM\n",
    "# ----------------------------------------------------------\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='tanh', input_shape=(sequence_length, len(feature_cols))))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 6) Evaluation on Test Set\n",
    "#    We'll compute R² in scaled space or we can invert.\n",
    "#    Then plot predictions vs actual.\n",
    "# ----------------------------------------------------------\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# MSE on test\n",
    "mse_test = np.mean((y_test - y_pred_test)**2)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "print(f\"Test MSE: {mse_test:.4f}\")\n",
    "print(f\"Test RMSE: {rmse_test:.4f}\")\n",
    "\n",
    "# R^2 in scaled domain\n",
    "r2 = r2_score(y_test, y_pred_test)\n",
    "print(f\"Test R2 score: {r2:.4f}\")\n",
    "\n",
    "# Visualization: Test Predictions vs. Actual (scaled)\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(y_test, label='Actual', linewidth=2)\n",
    "plt.plot(y_pred_test, label='Predicted', linewidth=2)\n",
    "plt.title(\"Test Set: Predicted vs Actual (Scaled)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Optional: Inverse transform\n",
    "# If we want the actual errors in original scale, we can do an inverse transform \n",
    "# for the target. We must feed back a shape that matches the scaled columns.\n",
    "# ----------------------------------------------------------\n",
    "y_test_reshaped = y_test.reshape(-1,1)\n",
    "y_pred_reshaped = y_pred_test.reshape(-1,1)\n",
    "\n",
    "# We'll build placeholders for [lag1, rolling24] just to do the inverse transform\n",
    "# though it's not always strictly necessary. We'll fill them with zeros.\n",
    "dummy_col_count = len(features_to_scale) - 1  # =2 if we have 3 features to scale\n",
    "dummy_zeros = np.zeros_like(y_test_reshaped).repeat(dummy_col_count, axis=1)\n",
    "\n",
    "test_scaled_for_inverse = np.hstack([y_test_reshaped, dummy_zeros])\n",
    "pred_scaled_for_inverse = np.hstack([y_pred_reshaped, dummy_zeros])\n",
    "\n",
    "# We'll also need to reconstruct them into a DataFrame with the same columns order\n",
    "# so that we can apply scaler.inverse_transform properly. \n",
    "inverse_df_cols = ['Total_consumption','lag1','rolling24']  # same order as features_to_scale\n",
    "test_scaled_df = pd.DataFrame(test_scaled_for_inverse, columns=inverse_df_cols)\n",
    "pred_scaled_df = pd.DataFrame(pred_scaled_for_inverse, columns=inverse_df_cols)\n",
    "\n",
    "y_test_original = scaler.inverse_transform(test_scaled_df)[:,0]  # first col is 'Total_consumption'\n",
    "y_pred_original = scaler.inverse_transform(pred_scaled_df)[:,0]\n",
    "\n",
    "# Compute errors in original scale\n",
    "errors = y_test_original - y_pred_original\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 7) Boxplot of errors by day of week\n",
    "# We'll use the day_of_week array for the test set (dow_test).\n",
    "# For each sample in X_test/y_test, we also have dow_test[i].\n",
    "# We group errors by day_of_week and produce a boxplot.\n",
    "# ----------------------------------------------------------\n",
    "df_errors = pd.DataFrame({\n",
    "    'day_of_week': dow_test,\n",
    "    'error': errors\n",
    "})\n",
    "\n",
    "# day_of_week is 0=Monday ... 6=Sunday\n",
    "plt.figure(figsize=(10,6))\n",
    "df_errors.boxplot(column='error', by='day_of_week', grid=False)\n",
    "plt.title(\"Errors (Actual - Predicted) by Day of Week\")\n",
    "plt.suptitle(\"\")  # Remove default title\n",
    "plt.xlabel(\"Day of Week (0=Mon ... 6=Sun)\")\n",
    "plt.ylabel(\"Error in Original Scale\")\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 8) Save the Model\n",
    "# ----------------------------------------------------------\n",
    "model.save(\"./models/total_consumption_lstm_model.keras\")\n",
    "print(\"Model saved as './models/total_consumption_lstm_model.keras'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
