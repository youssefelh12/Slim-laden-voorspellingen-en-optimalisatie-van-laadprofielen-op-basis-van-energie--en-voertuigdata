{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import holidays\n",
    "import calendar\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pmdarima import auto_arima\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "os.makedirs(\"results/daily3\", exist_ok=True)\n",
    "\n",
    "# ---------------------------\n",
    "# Load Daily Data\n",
    "# ---------------------------\n",
    "# The CSV is expected to have columns: Day, Chargers, Chargers achteraan, Grid Organi lbc, Solar\n",
    "df = pd.read_csv('api_data/aggregated_daily_measurements.csv')\n",
    "\n",
    "# Set 'Day' as datetime index\n",
    "df.set_index(\"Day\", inplace=True)\n",
    "df.index = pd.to_datetime(df.index)\n",
    "\n",
    "# Use only the \"Grid Organi lbc\" column for total consumption\n",
    "df['Total_consumption'] = df['Grid Organi lbc']\n",
    "\n",
    "# Drop unused columns\n",
    "df = df.drop(['Chargers', 'Chargers achteraan', 'Solar', 'Grid Organi lbc'], axis=1)\n",
    "\n",
    "# As the data is already daily, we directly copy it\n",
    "df_daily = df.copy()\n",
    "\n",
    "print(\"Dataset Information (Daily Data):\")\n",
    "print(f\"Time range: {df_daily.index.min()} to {df_daily.index.max()}\")\n",
    "print(f\"Total observations: {len(df_daily)}\")\n",
    "print(f\"Missing values: {df_daily['Total_consumption'].isna().sum()}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Transform Target: Log Consumption\n",
    "# ---------------------------\n",
    "# Ensure all consumption values are positive by shifting if needed\n",
    "shift_val = abs(df_daily[\"Total_consumption\"].min()) + 1  \n",
    "df_daily[\"log_consumption\"] = np.log(df_daily[\"Total_consumption\"] + shift_val)\n",
    "\n",
    "# ---------------------------\n",
    "# Feature Engineering for Daily Data\n",
    "# ---------------------------\n",
    "be_holidays = holidays.BE()  # Belgian holidays\n",
    "\n",
    "# Basic time features\n",
    "df_daily['day_of_week'] = df_daily.index.dayofweek\n",
    "df_daily['month'] = df_daily.index.month\n",
    "\n",
    "# Categorical features\n",
    "df_daily['is_weekend'] = df_daily['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "df_daily['is_festive'] = df_daily.index.to_series().apply(lambda x: 1 if x in be_holidays else 0)\n",
    "\n",
    "# Seasonal features\n",
    "df_daily['is_summer'] = df_daily.index.month.isin([6, 7, 8]).astype(int)\n",
    "df_daily['is_winter'] = df_daily.index.month.isin([12, 1, 2]).astype(int)\n",
    "\n",
    "# Cyclical features for day of week\n",
    "df_daily['day_of_week_sin'] = np.sin(2 * np.pi * df_daily['day_of_week'] / 7)\n",
    "df_daily['day_of_week_cos'] = np.cos(2 * np.pi * df_daily['day_of_week'] / 7)\n",
    "\n",
    "# Lagged features (avoid data leakage)\n",
    "df_daily['consumption_lag_1d'] = df_daily['Total_consumption'].shift(1)   # 1-day lag\n",
    "df_daily['consumption_lag_7d'] = df_daily['Total_consumption'].shift(7)   # 7-day lag\n",
    "\n",
    "# Additional lag features for monthly and yearly\n",
    "df_daily['consumption_lag_30d'] = df_daily['Total_consumption'].shift(30)   # Approx. monthly lag\n",
    "df_daily['consumption_lag_365d'] = df_daily['Total_consumption'].shift(365)   # Yearly lag\n",
    "\n",
    "# ---------------------------\n",
    "# Advanced Lag Features\n",
    "# ---------------------------\n",
    "# In addition to the basic lags above, we can engineer additional lag-based features to capture recurring patterns.\n",
    "# For instance, lagged consumption from 14 and 21 days ago may capture biweekly or other cyclical patterns.\n",
    "# Moving averages and rolling statistics help capture short-term momentum and volatility.\n",
    "df_daily['consumption_lag_14d'] = df_daily['Total_consumption'].shift(14)   # 14-day lag for biweekly patterns\n",
    "df_daily['consumption_lag_21d'] = df_daily['Total_consumption'].shift(21)   # 21-day lag for extended cycles\n",
    "df_daily['rolling_avg_3d'] = df_daily['Total_consumption'].rolling(window=3).mean()  # 3-day moving average\n",
    "df_daily['rolling_std_3d'] = df_daily['Total_consumption'].rolling(window=3).std()   # 3-day rolling standard deviation\n",
    "\n",
    "# Drop rows with NaN values resulting from lag features and rolling calculations\n",
    "df_daily.dropna(inplace=True)\n",
    "\n",
    "# ---------------------------\n",
    "# Visualization: Correlation Heatmap (Daily Data)\n",
    "# ---------------------------\n",
    "plt.figure(figsize=(12, 10))\n",
    "numerical_features = df_daily.select_dtypes(include=[np.number]).columns\n",
    "correlation = df_daily[numerical_features].corr()\n",
    "mask = np.triu(np.ones_like(correlation, dtype=bool))\n",
    "sns.heatmap(correlation, annot=True, fmt=\".2f\", cmap=\"coolwarm\", mask=mask, vmin=-1, vmax=1)\n",
    "plt.title(\"Daily Feature Correlation Heatmap\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/daily3/correlation_heatmap.png\")\n",
    "plt.close()\n",
    "\n",
    "# Plot original daily time series of Total Consumption\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(df_daily.index, df_daily['Total_consumption'], color='blue', alpha=0.6)\n",
    "plt.title('Daily Power Consumption Over Time', fontsize=14)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Power Consumption (kWh)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/daily3/time_series_plot.png\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nBasic Statistics (Total Consumption):\")\n",
    "print(df_daily['Total_consumption'].describe())\n",
    "\n",
    "# ---------------------------\n",
    "# Prepare Data for Modeling (Daily)\n",
    "# ---------------------------\n",
    "# Use the log-transformed consumption as the target\n",
    "target = \"log_consumption\"\n",
    "y_orig = df_daily[\"Total_consumption\"]\n",
    "\n",
    "# Define exogenous features suitable for daily forecasting\n",
    "exog_features = [\n",
    "    \"day_of_week_sin\", \"day_of_week_cos\", \"is_weekend\", \"is_festive\",\n",
    "    \"is_summer\", \"is_winter\", \"consumption_lag_1d\", \"consumption_lag_7d\",\n",
    "    \"consumption_lag_30d\", \"consumption_lag_365d\", \"consumption_lag_14d\", \"consumption_lag_21d\",\n",
    "    \"rolling_avg_3d\", \"rolling_std_3d\"\n",
    "]\n",
    "\n",
    "y = df_daily[target]\n",
    "exog = df_daily[exog_features]\n",
    "\n",
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "split_index = int(0.80 * len(df_daily))\n",
    "y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]\n",
    "exog_train, exog_test = exog.iloc[:split_index], exog.iloc[split_index:]\n",
    "y_orig_train, y_orig_test = y_orig.iloc[:split_index], y_orig.iloc[split_index:]\n",
    "\n",
    "print(f\"Training data from {df_daily.index[0].date()} to {df_daily.index[split_index-1].date()}\")\n",
    "print(f\"Testing data from {df_daily.index[split_index].date()} to {df_daily.index[-1].date()}\")\n",
    "\n",
    "# Normalize the exogenous features using training data\n",
    "scaler = StandardScaler()\n",
    "exog_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(exog_train),\n",
    "    columns=exog_train.columns,\n",
    "    index=exog_train.index\n",
    ")\n",
    "exog_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(exog_test),\n",
    "    columns=exog_test.columns,\n",
    "    index=exog_test.index\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Find Optimal ARIMA Parameters (Daily)\n",
    "# ---------------------------\n",
    "print(\"Finding optimal ARIMA parameters with limited search space...\")\n",
    "try:\n",
    "    model_auto = auto_arima(\n",
    "        y_train,\n",
    "        exogenous=exog_train_scaled,\n",
    "        seasonal=True,\n",
    "        m=7,  # Weekly seasonality for daily data\n",
    "        trace=True,\n",
    "        error_action=\"ignore\",\n",
    "        suppress_warnings=True,\n",
    "        stepwise=True,\n",
    "        max_order=3,\n",
    "        max_p=2,\n",
    "        max_q=2,\n",
    "        max_d=1,\n",
    "        max_P=1,\n",
    "        max_Q=1,\n",
    "        max_D=1,\n",
    "        start_p=1,\n",
    "        start_q=1,\n",
    "        start_P=1,\n",
    "        start_Q=1,\n",
    "        information_criterion='aic',\n",
    "        maxiter=50,\n",
    "        method='lbfgs',\n",
    "        n_jobs=1\n",
    "    )\n",
    "    \n",
    "    order = model_auto.order\n",
    "    seasonal_order = model_auto.seasonal_order\n",
    "    \n",
    "    print(\"Optimal ARIMA order:\", order)\n",
    "    print(\"Optimal Seasonal order:\", seasonal_order)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Auto ARIMA failed with error: {e}\")\n",
    "    print(\"Using predefined ARIMA parameters instead...\")\n",
    "    order = (1, 1, 1)\n",
    "    seasonal_order = (1, 0, 0, 7)\n",
    "\n",
    "# ---------------------------\n",
    "# Fit SARIMAX Model with a Constant Trend (Daily)\n",
    "# ---------------------------\n",
    "print(\"\\nFitting SARIMAX model...\")\n",
    "try:\n",
    "    model = SARIMAX(\n",
    "        y_train,\n",
    "        exog=exog_train_scaled,\n",
    "        order=order,\n",
    "        seasonal_order=seasonal_order,\n",
    "        trend='c',  # Include a constant trend\n",
    "        enforce_stationarity=False,\n",
    "        enforce_invertibility=False\n",
    "    )\n",
    "    model_fit = model.fit(disp=False, maxiter=200, method='lbfgs')\n",
    "    \n",
    "    print(\"\\nModel Summary:\")\n",
    "    print(model_fit.summary().tables[0].as_text())\n",
    "    print(model_fit.summary().tables[1].as_text())\n",
    "    \n",
    "    # Forecast on the log-transformed scale\n",
    "    forecast_obj = model_fit.get_forecast(steps=len(y_test), exog=exog_test_scaled)\n",
    "    y_pred_log = forecast_obj.predicted_mean\n",
    "    y_pred_log.index = y_test.index\n",
    "\n",
    "except MemoryError:\n",
    "    print(\"Memory error during SARIMAX fitting. Using a simplified approach...\")\n",
    "    window_size = 2000\n",
    "    y_pred_log = []\n",
    "    for i in range(0, len(y_test), window_size):\n",
    "        end_idx = min(i + window_size, len(y_test))\n",
    "        subset_train = y_train.iloc[-10000:] if len(y_train) > 10000 else y_train\n",
    "        subset_exog_train = exog_train_scaled.iloc[-10000:] if len(exog_train_scaled) > 10000 else exog_train_scaled\n",
    "        \n",
    "        subset_model = SARIMAX(\n",
    "            subset_train,\n",
    "            exog=subset_exog_train,\n",
    "            order=(1, 1, 1),\n",
    "            seasonal_order=(1, 0, 0, 7),\n",
    "            trend='c',\n",
    "            enforce_stationarity=False,\n",
    "            enforce_invertibility=False\n",
    "        )\n",
    "        subset_fit = subset_model.fit(disp=False, maxiter=100, method='lbfgs')\n",
    "        subset_pred = subset_fit.get_forecast(\n",
    "            steps=end_idx - i,\n",
    "            exog=exog_test_scaled.iloc[i:end_idx]\n",
    "        )\n",
    "        y_pred_log.extend(subset_pred.predicted_mean.tolist())\n",
    "    \n",
    "    y_pred_log = pd.Series(y_pred_log, index=y_test.index)\n",
    "\n",
    "# ---------------------------\n",
    "# Invert the Log Transformation (Daily)\n",
    "# ---------------------------\n",
    "y_pred = np.exp(y_pred_log) - shift_val\n",
    "\n",
    "# ---------------------------\n",
    "# Compute Performance Metrics on Original Scale\n",
    "# ---------------------------\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    epsilon = 1e-10\n",
    "    return np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n",
    "\n",
    "def calculate_smape(y_true, y_pred):\n",
    "    denominator = np.abs(y_true) + np.abs(y_pred) + 1e-10\n",
    "    return 100 * np.mean(2 * np.abs(y_pred - y_true) / denominator)\n",
    "\n",
    "mae = mean_absolute_error(y_orig_test, y_pred)\n",
    "mse = mean_squared_error(y_orig_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mape = calculate_mape(y_orig_test.values, y_pred.values)\n",
    "smape = calculate_smape(y_orig_test.values, y_pred.values)\n",
    "r2 = r2_score(y_orig_test, y_pred)\n",
    "\n",
    "print(\"\\nModel Performance Metrics (Original Scale):\")\n",
    "print(f\"MAE:   {mae:.4f}\")\n",
    "print(f\"MSE:   {mse:.4f}\")\n",
    "print(f\"RMSE:  {rmse:.4f}\")\n",
    "print(f\"MAPE:  {mape:.4f}%\")\n",
    "print(f\"sMAPE: {smape:.4f}%\")\n",
    "print(f\"R²:    {r2:.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Create Forecast DataFrame and Save Predictions (Daily)\n",
    "# ---------------------------\n",
    "forecast_df = pd.DataFrame({\n",
    "    \"Real_Consumption_kWh\": y_orig_test,\n",
    "    \"Predicted_Consumption_kWh\": y_pred\n",
    "}, index=y_orig_test.index)\n",
    "\n",
    "forecast_df[\"Difference\"] = forecast_df[\"Predicted_Consumption_kWh\"] - forecast_df[\"Real_Consumption_kWh\"]\n",
    "forecast_df[\"Absolute_Error\"] = abs(forecast_df[\"Difference\"])\n",
    "forecast_df[\"Percent_Error\"] = (forecast_df[\"Absolute_Error\"] / (forecast_df[\"Real_Consumption_kWh\"].abs() + 1e-10)) * 100\n",
    "forecast_df = forecast_df.round(4)\n",
    "forecast_df.to_csv(\"results/daily3/predicted_values_kwh.csv\")\n",
    "print(\"\\nPredicted values have been saved to 'results/daily3/predicted_values_kwh.csv'\")\n",
    "\n",
    "# ---------------------------\n",
    "# Feature Importance Analysis (Daily)\n",
    "# ---------------------------\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': exog_features,\n",
    "    'Correlation': [abs(df_daily[feature].corr(df_daily[target])) for feature in exog_features]\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('Correlation', ascending=False)\n",
    "print(\"\\nFeature Importance by Correlation with Target (Log Scale):\")\n",
    "print(feature_importance.head(10))\n",
    "feature_importance.to_csv(\"results/daily3/feature_importance.csv\", index=False)\n",
    "\n",
    "# ---------------------------\n",
    "# Visualization: Forecast Comparison (Daily)\n",
    "# ---------------------------\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(forecast_df.index, forecast_df[\"Real_Consumption_kWh\"], label=\"Actual\", color=\"blue\", alpha=0.6, linewidth=1)\n",
    "plt.plot(forecast_df.index, forecast_df[\"Predicted_Consumption_kWh\"], label=\"Predicted\", color=\"red\", alpha=0.6, linewidth=1)\n",
    "plt.title(\"Actual vs Predicted Daily Power Consumption\", fontsize=14)\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.ylabel(\"Consumption (kWh)\", fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/daily3/forecast_comparison.png\")\n",
    "plt.close()\n",
    "\n",
    "# Plot the residuals\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(forecast_df.index, forecast_df[\"Difference\"], color='green', alpha=0.6)\n",
    "plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
    "plt.title(\"Daily Prediction Residuals (Predicted - Actual)\", fontsize=14)\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.ylabel(\"Residual\", fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/daily3/residuals.png\")\n",
    "plt.close()\n",
    "\n",
    "# Plot error distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(forecast_df[\"Difference\"], bins=30, alpha=0.7, color='skyblue')\n",
    "plt.title(\"Distribution of Daily Prediction Errors\", fontsize=14)\n",
    "plt.xlabel(\"Prediction Error\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/daily3/error_distribution.png\")\n",
    "plt.close()\n",
    "\n",
    "# Remove index name to avoid ambiguity when grouping\n",
    "forecast_df.index.name = None\n",
    "\n",
    "# Add additional column for error analysis: day_of_week\n",
    "forecast_df['Day_of_Week'] = forecast_df.index.dayofweek\n",
    "\n",
    "# Error by day of week\n",
    "plt.figure(figsize=(12, 6))\n",
    "day_error = forecast_df.groupby('Day_of_Week')['Absolute_Error'].mean()\n",
    "sns.barplot(x=day_error.index, y=day_error.values)\n",
    "plt.title('Average Prediction Error by Day of Week', fontsize=14)\n",
    "plt.xlabel('Day of Week (0=Monday, 6=Sunday)', fontsize=12)\n",
    "plt.ylabel('Mean Absolute Error', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/daily3/error_by_day.png\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nAll daily visualizations have been saved in the 'results/daily3' directory.\")\n",
    "print(\"Daily forecasting analysis complete.\")\n",
    "\n",
    "# ---------------------------\n",
    "# Boxplot: Comparison of Real vs Forecasted Values\n",
    "# ---------------------------\n",
    "# Melt the DataFrame to convert it into a long format suitable for a boxplot.\n",
    "melted_df = forecast_df[['Real_Consumption_kWh', 'Predicted_Consumption_kWh']].reset_index().melt(\n",
    "    id_vars='index', \n",
    "    value_vars=['Real_Consumption_kWh', 'Predicted_Consumption_kWh'],\n",
    "    var_name='Type',\n",
    "    value_name='Consumption'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Type', y='Consumption', data=melted_df)\n",
    "plt.title(\"Comparison of Real vs Forecasted Daily Consumption\")\n",
    "plt.xlabel(\"Consumption Type\")\n",
    "plt.ylabel(\"Daily Consumption (kWh)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
